---
title: 'Materials for Modal Spaces'
author: "Jonathan Phillips & Eli Hecht"
date: "December 2022"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(lme4)
library(knitr)
library(ggrepel)

knitr::opts_chunk$set(echo = FALSE,dpi=300,fig.width=7)
```

## Study 1: Possibility Generation

### Participants

```{r participantspg,echo=FALSE}
# read in Study 1a-c data
pg1 <- read.csv("data/pg1.csv")
pg2 <- read.csv("data/pg2.csv")
pg3 <- read.csv("data/pg3.csv")

# add id column
pg1 <- pg1 %>% rownames_to_column(var = "id") %>% 
  mutate(id = as.numeric(id))
pg2 <- pg2 %>% rownames_to_column(var = "id") %>%
  mutate(id = nrow(pg1) + as.numeric(id))
pg3 <- pg3 %>% rownames_to_column(var = "id") %>% 
  mutate(id = nrow(pg1) + nrow(pg2) + as.numeric(id))

# join data from three studies into one table
pgW <- bind_rows(pg1, pg2, pg3) %>% 
  filter(Progress == 100)

## issue with 25 being entered twice for one participant's age
pgW$age[pgW$age==2525] <- 25
```

Across three studies ($N_{1}$ = `r length(unique(pg1$id))`, $N_{2}$ = `r length(unique(pg2$id))`, $N_{3}$ = `r length(unique(pg3$id))`) we collected a total sample of `r length(unique(pgW$id))` participants ($M_{age}$ = `r round(mean(pgW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(pgW$age,na.rm=T),2)`; `r table(pgW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

### Procedure

In each study, participants were presented with six stories in random order, each describing an agent having to make a decision. All 18 stories are presented in Table 1 in the appendix.

For each story, participants were asked:

|     In this situation, what are some things you believe [Agent] could do? Please list 5
| \ 
After listing their responses for each scenario, participants were then asked to rate each of their answers on three 100 point
scales.

|     Consider one of the things you believed [Agent] could do: [Participant response].

|     *Probability Question:* How likely is it that [Agent] will do that thing?

|     *Morality Question:* How morally acceptable is it that [Agent] will do that thing?

|     *Normality Question:* How normal is it that [Agent] will do that thing?

### Results

```{r tidypg,echo=FALSE, warning=FALSE, message=FALSE}

# converts the data from wide, with one row per participant, to long, with one row for every response
pgL <- pgW %>% select(id, S1A1_4:S18A5_6, -c(demo:ses, contains("FL"), contains("."))) %>% 
  pivot_longer(-id, names_to = "question", values_to = "response",
               values_drop_na = T) %>% 
  separate(question, into = c("context", "answer"), sep = "A") %>% 
  separate(answer, into = c("answer", "question"), sep = "_") %>% 
  mutate(across(2:4, parse_number))  %>% 
  mutate(question = if_else(question %in% c(4, 7), "Probability",
                        if_else(question %in% c(5, 8), "Morality", "Normality")))

# removes participants who clearly didn't take survey seriously/put in nonsensical answers
pgL <- pgL %>% filter(!id %in% c(57, 88, 91, 135, 151, 153, 158, 189, 248, 229))

# removes answers for any scenario sets that don't have full responses (3 ratings for each of 5 questions)
pgL <- pgL %>% group_by(id, context) %>%
    filter(n()==15)

## creates an score for the average of the three judgments
pgL <- pgL %>% pivot_wider(names_from = question, values_from = response ) %>%
                mutate(avg=(Normality+Probability+Morality)/3) %>%
                pivot_longer(cols=c(Normality,Morality,Probability,avg),
                             names_to = "question",values_to ="response")

# percentage of time that a participant rates each answer for a given scenario highest. ties go to the first answer
perBest <- pgL %>% filter(question=="avg") %>%
  group_by(id, context) %>% 
  mutate(rank = rank(-response, ties.method = "first")) %>% 
  group_by(answer) %>% 
  summarise(numBest = sum(rank == 1), .groups = "drop") %>% 
  ungroup() %>% 
  mutate(perBest = numBest/sum(numBest)) 

# spreads out data into 1 row per participant response with 3 ratings in separate columns, as well as the average
pgPredict <- pgL %>% pivot_wider(names_from = "question", values_from = "response")

# Here we predict the order in which an answer was generated based on how highly the answer is rated
if(file.exists("lmerOutputs/pgPredictP.rda")){
  # saves computational time to just read these tables instead of recalculating them every time
  pgPredictN <- readRDS("lmerOutputs/pgPredictN.rda")
  pgPredictP <- readRDS("lmerOutputs/pgPredictP.rda")
  pgPredictM <- readRDS("lmerOutputs/pgPredictM.rda")
} else{
    lmerSamp_Full <- lmer(as.numeric(answer) ~ scale(Morality) + scale(Normality) +  scale(Probability) + 
                         (1|context) + 
                         (scale(Morality) + scale(Normality) +  scale(Probability) |id),
                       data=pgPredict)
    
    # lesion out probability
    lmerSamp_P <- lmer(as.numeric(answer) ~ scale(Morality) + scale(Normality) + 
                         (1|context) + 
                         (scale(Morality) + scale(Normality) +  scale(Probability) |id),
                       data=pgPredict)
    # lesion out morality
    lmerSamp_M <- lmer(as.numeric(answer) ~ scale(Probability) + scale(Normality) + 
                         (1|context) + 
                         (scale(Morality) + scale(Normality) +  scale(Probability) |id),
                       data=pgPredict)
    # lesion out normality
      lmerSamp_N <- lmer(as.numeric(answer) ~ scale(Morality) + scale(Probability) + 
                         (1|context) + 
                         (scale(Morality) + scale(Normality) +  scale(Probability) |id),
                       data=pgPredict)
      
  pgPredictP <- anova(lmerSamp_Full,lmerSamp_P)
      saveRDS(pgPredictP, file = "lmerOutputs/pgPredictP.rda")
  pgPredictM <- anova(lmerSamp_Full,lmerSamp_M)
      saveRDS(pgPredictM, file = "lmerOutputs/pgPredictM.rda")
  pgPredictN <- anova(lmerSamp_Full,lmerSamp_N)
    saveRDS(pgPredictN, file = "lmerOutputs/pgPredictN.rda")
  
}
#  anova predicting answer number by average rating of an answer
pgPredict <- summary(aov(lm(as.numeric(answer) ~ avg, data=pgPredict)))


# Here we investigate the relationship between normality ratings and probability and morality ratings:
if(file.exists("lmerOutputs/pgNormInt.rda")){
  # saves computational time to just read these tables instead of recalculating them every time
  pgNormInt <- readRDS("lmerOutputs/pgNormInt.rda")
  pgNormP <- readRDS("lmerOutputs/pgNormP.rda")
  pgNormM <- readRDS("lmerOutputs/pgNormM.rda")
} else{
  # predicting normality from interaction of morality and probability
  lmerNorm_Full <- lmer(scale(Normality) ~ scale(Morality) * scale(Probability) + (1|context) +
                   (scale(Probability) * scale(Morality) |id), data=pgPredict)
  # predicting normality with morality and probability as separate fixed effects
  lmerNorm_Main <- lmer(scale(Normality) ~ scale(Morality) + scale(Probability) + (1|context) +
                   (scale(Probability) * scale(Morality) |id), data=pgPredict)
  # model with probability removed
  lmerNorm_P <- lmer(scale(Normality) ~ scale(Morality) + (1|context) +
                   (scale(Probability) * scale(Morality) |id), data=pgPredict)
  # model with morality removed
  lmerNorm_M <- lmer(scale(Normality) ~ scale(Probability) + (1|context) +
                   (scale(Probability) * scale(Morality) |id), data=pgPredict)
  

  pgNormInt <- anova(lmerNorm_Full,lmerNorm_Main) # effect of interaction
    saveRDS(pgNormInt, file = "lmerOutputs/pgNormInt.rda") # these just save the models so you don't have to rerun later
  pgNormP <- anova(lmerNorm_Main,lmerNorm_P) # effect of probability
    saveRDS(pgNormP, file = "lmerOutputs/pgNormP.rda")
  pgNormM <- anova(lmerNorm_Main,lmerNorm_M) # effect of morality
    saveRDS(pgNormM, file = "lmerOutputs/pgNormM.rda")
  
}
```

Participants' first responses for each scenario tended to be the one they rated highest, or tied for highest (`r round(perBest[[1,3]] * 100, 1)`% of the time).

The better an option is, the earlier it tended to be generated by a participant ($F$(`r pgPredict[[1]][["Df"]][1]`$) =$ `r round(pgPredict[[1]][["F value"]][1],2)`, $p <$ `r max(.001, round(pgPredict[[1]][["Pr(>F)"]][1],3))`). Further, independent of normality and morality, the more probable a response was, the more likely it was to be generated earlier ($\chi^2$(`r pgPredictP$Df[[2]]`) = `r pgPredictP$Chisq[[2]] %>% round(2) %>% format(scientific=F)`, $p <$ `r max(.001, round(pgPredictP$"Pr(>Chisq)"[[2]], 3))`). The same is true for morality, where independent of the other ratings, the more moral a possibility was, the more likely it was to be generated earlier ($\chi^2$(`r pgPredictM$Df[[2]]`) = `r round(pgPredictM$Chisq[[2]], 2) %>% format(scientific=F)`, $p <$ `r max(.001, round(pgPredictM$"Pr(>Chisq)"[[2]], 3))`). Normality was not found to have a similar independent predictive effect, ($\chi^2$(`r pgPredictN$Df[[2]]`) = `r round(pgPredictN$Chisq[[2]], 3)`, $p =$ `r max(.001, round(pgPredictN$"Pr(>Chisq)"[[2]], 3))`). This is likely because, as will be seen below, normality is jointly predicted by probability and morality, much of the variance in answer generation number is already accounted for by these ratings.

We sought to replicate the finding of Bear and Knobe (2017) that normality judgments are predicted by judgments of morality and probability. First we constructed a linear mixed-effects model to predict normality ratings with probability and morality ratings as predictors and fixed effects for context and [not positive on how to refer to the structure of the model]. We further found that both ratings were independently predictive of normality. The model performed worse when probability was removed ($\chi^2$(`r pgNormP$Df[[2]]`) = `r round(pgNormP$Chisq[[2]], 3)`, $p <$ `r max(.001, round(pgNormP$"Pr(>Chisq)"[[2]], 3))`), and when morality was removed ($\chi^2$(`r pgNormM$Df[[2]]`) = `r round(pgNormM$Chisq[[2]], 3)`, $p <$ `r max(.001, round(pgNormM$"Pr(>Chisq)"[[2]], 3))`). Importantly, the model performed significantly worse when the interaction between morality and probability were removed ($\chi^2$(`r pgNormInt$Df[[2]]`) = `r round(pgNormInt$Chisq[[2]], 3)`, $p <$ `r max(.001, round(pgNormInt$"Pr(>Chisq)"[[2]], 3))`). 

```{r fig1, echo=F, warning=F, message=F, fig.width=6.5,fig.height=5.25}

## Figure 1
fig1 <- pgL %>% filter(question!="avg") %>%
  mutate(question = factor(question)) %>%
  mutate(answer=factor(answer)) %>%
  ggplot(aes(x=answer, y=response, fill=question)) +
  coord_cartesian(xlim = c(0.75, 5.25), ylim = c(0,100)) +
  geom_boxplot(position="dodge",alpha=.6,outlier.alpha = 0) +
  geom_point(aes(color=question),position=position_jitterdodge(jitter.width = .35), alpha = 0.1) +
  labs(x = "Generation number", y = "Value of option generated", fill="Rating:",color="Rating:") +
  geom_smooth(aes(y=response,x=as.numeric(answer),color=question),method="lm",position = position_dodge(width = .75)) +
  theme_bw() +
  theme(
    plot.background = element_blank()
    ,legend.position = "top"
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
    ,axis.text.y=element_text(size=rel(1.4))
    ,axis.title=element_text(size=rel(1.5),vjust=.9)
    ,axis.text.x=element_text(size=rel(1.5))
    ,axis.ticks = element_blank()
    ,legend.text = element_text(size=rel(1.5))
    ,legend.title = element_text(size=rel(1.5))
  )

ggsave(fig1,file="figs/fig1.png",width = 10,height = 7)

print(fig1)

```

## Grouping participant responses

```{r tidycoded, warnings = F, echo=F,message=F}
# rater 1 codings
coding1 <- read.csv("manualCoding/texts_coding_1.csv") %>% 
  rename(coding1 = group)
# rater 2 codings
coding2 <- read.csv("manualCoding/texts_coding_2.csv") %>% 
  rename(coding2 = group)

coded <- full_join(coding1, coding2) 

# calculates agreement between raters
agreement <- coded %>% summarise(agree = sum(coding1==coding2),
                    disagree=sum(coding1 != coding2)) %>% 
  mutate(sum = agree+disagree)

# rater 3 rated in cases of disagreement between the first two raters
coding3 <- read.csv("manualCoding/texts_coding_3.csv") %>% 
  rename(codingFinal = coded.final)

coded <- coded %>% full_join(coding3 %>% select(-coded1, -coded2)) %>% 
  mutate(group = if_else(is.na(codingFinal), coding1, codingFinal)) %>% 
  select(-(3:5))

coded <- coded %>% separate(scenario_id_fr, into = c("context", "id", "answer"), sep = "_") %>% 
  mutate(across(1:3, as.numeric)) %>% 
  right_join(pgL %>% filter(question!="avg") %>% group_by(context, id, answer) %>% summarise(value = mean(response)))

codingKey <- read.csv("materials/textsCodingKey.csv") %>% 
  rename(group = X)

# coded includes participant answers + ratings as well as manual coded groups
coded <- codingKey %>%
  pivot_longer(-group, names_to = "context", values_to = "groupText") %>% 
  mutate(across(1:2, parse_number)) %>%
  unite(col = "context_group", context, group, sep = "_") %>% 
  filter(groupText != "") %>%
  right_join(coded %>% mutate(context_group = paste(context, group, sep = "_"))) 
```

Two raters manually coded each of the `r nrow(coding1)` participant responses into 13-18 distinct action categories for each context The criterion for a grouping was that at least three participants generated an action within that category. Two raters independently grouped participant responses with an inter-rater agreement of `r round(agreement[[1,1]]/(agreement[[1,1]]+agreement[[1,2]]), 1) * 100`%. A third rater determined final results in cases of disagreements. Action categories for each context are presented in Table 2 in the appendix.

### Results

```{r codingresults, warnings = F, echo=F,message=F}
rm(coding1, coding2, coding3)

# ranks groups for each context by number of answers 
bestGroups <- coded %>% group_by(context, group) %>% 
  summarise(n = n(), mean = mean(value)) %>%
  filter(group != 0) %>% 
  group_by(context) %>% 
  mutate(rank = rank(-n, ties.method = "f")) %>% 
  unite("context_group", context, group)

# adds columns with information about whether a given answer is in the top 1 or 3 most picked groups for that context
coded <- coded %>% unite("context_group", context, group, remove = F) %>%
  ungroup() %>% 
  mutate(best = context_group %in%
           (bestGroups %>% filter(rank == 1) %>% pull(context_group)), 
         top3 = context_group %in%
           (bestGroups %>% filter(rank <= 3) %>% pull(context_group)))

# percentage of participants who didn't come up with any of the most picked answers for that context
perBest <- coded %>% 
  group_by(context, id) %>% 
  summarise(noneBest = !any(best),
            noneTop3 = !any(top3)) %>%
  ungroup() %>% 
  summarise(noneBest = sum(noneBest)/n(),
            noneTop3 = sum(noneTop3)/n())


# for each context do an independent samples t test comparing values of answers in the 3 most commonly used action categories to all other answers
t_table <- data.frame(context = numeric(0), t = numeric(0), df = numeric(0), p = numeric(0), mean_top3 = numeric(0), sd_top3 = numeric(0), mean_other = numeric(0), sd_other =numeric(0))
for (c in 1:18) {
  test <- coded %>% 
    filter(context == c) %>%
    with(t.test(value ~ top3))
  
  t_table <- bind_rows(t_table, 
        data.frame(context = c, t = test$statistic, df = test$parameter,
           p = test$p.value,
           mean_top3 = coded %>% filter(context == c, top3) %>% summarise(mean(value)) %>% pull(),
           sd_top3 = coded %>% filter(context == c, top3) %>% summarise(sd(value)) %>% pull(),
           mean_other = coded %>% filter(context == c, !top3) %>% summarise(mean(value)) %>% pull(),
           sd_other = coded %>% filter(context == c, !top3) %>% summarise(sd(value)) %>% pull(), n_top3 = coded %>% filter(context == c, top3) %>% nrow(), n_other = coded %>% filter(context == c, !top3) %>% nrow()))
}
rm(test)
```

We found striking convergence across participant's answers within each context. Only `r round(summarise(coded, otherPer = sum(group==0)/n())[[1]] * 100)`% of answers were labeled as 'other.' Across contexts, only `r round(perBest[[1,2]] * 100)`% of participants did not come up with any of the 3 most common answers in their set of 5 possible actions, and only `r round(perBest[[1,1]] * 100)`% of participants didn't put down the most popular answer for that context.\
Not only did participants converge on a relatively small set of action categories (13-18), but the most common answers also tended to be rated highly. Across contexts, the actions in the three most common categories were rated higher ($M =$ `r coded %>% filter(top3) %>% summarise(mean(value)) %>% pull() %>% round(1)`, $SD =$ `r coded %>% filter(top3) %>% summarise(sd(value)) %>% pull() %>% round(1)`) than other actions ($M =$ `r coded %>% filter(!top3) %>% summarise(mean(value)) %>% pull() %>% round(1)`, $SD =$ `r coded %>% filter(!top3) %>% summarise(sd(value)) %>% pull() %>% round(1)`).\
This pattern held true within each context. For each context, we performed an independent samples t-test comparing average ratings for answers in the three most popular action categories to answers outside of these categories. For all but `r t_table %>% summarise(sum(p > .001)) %>% pull()` of the 18 contexts t-values were in the expected direction, with $p<.001$.


```{r codingHistograms, results = "asis", warning=F,message=F}

# histogram showing the different action categories for a specific context, including information with number and average rating of actions in each category
for (c in c(13)) { ## You can add contexts here if you want to see the corresponding graphs
  codedHist <- coded %>% 
  group_by(context, group, groupText) %>%
  summarise(count = n(), pgAvg = mean(value)) %>%
  filter(context == c) %>%
  group_by(context) %>% 
  mutate(rank = rank(-count, ties.method = "f"), groupText = as.factor(groupText)) %>% 
  ggplot(aes(x = reorder(groupText, rank), y = count, label = groupText,
             fill = pgAvg)) +
  labs(x = "Action category", y = "Number of participant responses", fill = "Average rating", title = paste("Context", c)) +
  geom_col() +
  theme_bw() +
  theme(
    plot.background = element_blank()
    #,legend.position = "none"
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
    ,axis.text.y=element_text(size=rel(1.4))
    #,axis.title=element_blank()
    #,axis.title.y=element_blank()
    ,axis.text.x=element_text(size=rel(1.5), angle = 60, hjust = 1)
    ,axis.title.x = element_text(size=rel(2))
    ,axis.ticks = element_blank()
  )
  assign(paste0("codedHist", c), codedHist)
}

plot(codedHist13)

# ggsave(codedHist13,file="figs/codedHist13.jpg",width = 12,height = 10)



```

## Study 2: Action Ratings

For each context, we created a list of six actions the agent could possibly do. These actions were selected to vary widely along the space of options that come to mind. The six actions for each context are presented in Table 1 in the appendix.

### Participants

```{r participantsev, echo=F,warning=F,message=F}
evW <- read.csv("data/ev.csv") %>% 
  rownames_to_column("id")
```

To verify that the actions we came up with did in fact vary as intended, we collected a sample of `r length(unique(evW$id))` participants ($M_{age}$ = `r round(mean(evW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(evW$age,na.rm=T),2)`; `r table(evW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

### Procedure

Each participant viewed a random subset of six of the eighteen contexts. For each context, in addition to reading the story, they were randomly assigned one of the six actions and told that the agent was considering that action. They were then asked to rate that action on the same three 100-point scales used earlier, measuring the probability, morality and normality of the action.

### Results
```{r tidyev, echo=F,warning=F,message=F}
# creates long dataframe
evL <- evW %>% filter(Progress==100) %>% 
  select(id, S1_4:S18_6) %>% 
  pivot_longer(-id, names_to = "context_question", values_to = "response",
               values_drop_na = T) %>% 
  separate(context_question, into = c("context", "question"), sep = "_") %>% 
  mutate(across(1:3, parse_number)) %>% 
  mutate(question = if_else(question == 4, "Probability", 
                            if_else(question == 5, "Morality", "Normality")))

evL <- evW %>% select(id, S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action") %>% 
  mutate(context = parse_number(context),
         id = as.numeric(id)) %>% 
  left_join(evL, .) %>%
  relocate(action, .after = "context") %>%
  pivot_wider(names_from = question, values_from = response ) %>%
                mutate(avg=(Normality+Probability+Morality)/3) %>% # this creates the average for all three judgments
                pivot_longer(cols=c(Normality,Morality,Probability,avg),
                             names_to = "question",values_to ="response")

# creates a summary table with average ratings for each action
evSum <- evL %>% 
  group_by(context, action, question) %>%
    summarise(mean = mean(response), .groups = "drop") %>%
    as.data.frame() %>% 
    group_by(context) %>% 
    mutate(actionNumb = paste(context, as.integer(as.factor(action)), sep = "_") ) %>% 
  relocate(actionNumb, .after = "context") 

```

```{r histograms, echo = F}
# histogram showing the distribution of participant response ratings along with the average ratings for the events we gave to participants
histograms18 <- pgL %>% #filter(question!="avg") %>%
  mutate(question = factor(question),
         question = factor(c("Value","Morality","Normality","Probability")[question])) %>%
    ggplot(aes(x = response, fill = question, color = question)) +
      geom_histogram(position = "identity",
                     alpha = 0.33,
                     bins = 15) +
      geom_jitter(data = evSum %>% #filter(question!="avg") %>%
                          mutate(question = factor(question),
                          question = factor(c("Value","Morality","Normality","Probability")[question]))
                  ,size = 1.5
                  ,aes(x = mean, y = -250)
                  ,width = 0, height = 100
                  ,alpha = .33) +
      facet_grid(~ question) +
    theme_bw() +
    theme(
      plot.background = element_blank()
      ,panel.grid.major = element_blank()
      ,panel.grid.minor = element_blank()
      ,legend.title=element_blank()
      ,legend.text=element_text(size=rel(1.4))
      ,axis.text.y=element_text(size=rel(1.5))
      ,axis.title.y=element_text(vjust=.9)
      ,axis.ticks = element_blank()
      ,strip.text=element_text(size=rel(1))
      ,axis.title=element_text(size=rel(1))
     ,legend.position = "none"
    ) +
    xlab("Value") +
    ylab("Count")

ggsave(histograms18,filename = "figs/figS2.jpg", width = 7, height = 5,units = "in")

```
Actions varied widely across contexts, ranging from an average rating of `r evSum %>% filter(question == 'avg') %>% pull(mean) %>% min() %>% round(2)` at minimum to `r evSum %>% filter(question == 'avg') %>% pull(mean) %>% max() %>% round(2)` at maximum. The average rating was `r evSum %>% filter(question == 'avg') %>% pull(mean) %>% mean() %>% round(2)` with a standard deviation of `r evSum %>% filter(question == 'avg') %>% pull(mean) %>% sd() %>% round(2)`.

### Calculating the modal distance
In order to predict participants' higher level judgments, we developed a novel measure that compared ratings for each action against the set of all participant generated responses for the context. This value, what we're referring to as the *modal distance*, is the proportion of participant generated responses for the context (from Study 1) that had a higher average rating than the average rating for a given action (from Study 2). The less normal an answer is, the higher its modal distance value will be (i.e. the more distant it is from the center of the contextual modal space). Importantly, the value is context dependent: an action with an average normality rating of 50 will have a lower modal distance in a context with skewed positive responses than in a context with a more uniform distribution. This value will be used to predict responses in all subsequent studies. \ 
We also sought to see whether each individual judgment (probability, morality and normality) was separately predictive of high-level judgments. We recalculated the modal distance for each action, except instead of using the averages of probability, morality and normality as the value score for each participant-generated and actual action, we calculated three different scores, one for each type of judgment. This gave use a probability distance, a morality distance and normality distance for each action.

```{r modalDistance, message=F,warning=F,echo=F}

## Figure showing how modal distance is calculated
density13 <-  pgL %>% 
  filter(context == 13) %>% ## just using our example context
  filter(question == "avg") %>% ## using the average measure
  mutate(disvalue=100-response) %>%
    ggplot(aes(x = disvalue),color="grey") +
      geom_histogram(aes(y = ..density..),
                     position = "identity",
                     alpha = 0.33,
                     bins = 20) +
      geom_density(lwd = 1, alpha = 0.25, fill="grey") +
      geom_label_repel(data = evSum %>% filter(context == 13) %>% ## again only context 13 for the actual actions
                                     filter(question=="avg") %>% ## again using the average of the judgments
                                     mutate(disvalue=100-mean,
                                            event=factor(action),
                                            shortevent = factor(c("blame maid of honor",
                                                                  "borrow ring",
                                                                  "call taxi",
                                                                  "run away",
                                                                  "steal ring",
                                                                  "tell sister")[event])
                                            ), 
                 aes(x = disvalue, y = -0.006, 
                     label = shortevent, color=c("black","black","black","red","black","black")),
                 check_overlap=TRUE,
                 position=position_jitter(width = 0, height = .001,seed=4),
                 segment.colour = NA
                ) +
    scale_color_manual(values=c("black","red")) +
    xlab("Disvalue Score") +
    ylab("Density") +
    #ggtitle("Context 13 (Daniel the Ring Bearer)") +
    theme_bw() +
    theme(
      plot.background = element_blank()
      ,panel.grid.major = element_blank()
      ,panel.grid.minor = element_blank()
      ,legend.title=element_blank()
      ,legend.text=element_text(size=rel(1.25))
      ,axis.text.y=element_text(size=rel(1))
      ,axis.title.x=element_text(size=rel(1.25))
      ,axis.title.y=element_text(vjust=.9,size=rel(1.25))
      ,axis.ticks = element_blank()
      ,strip.text=element_text(size=rel(1))
      ,axis.title=element_text(size=rel(1))
     ,legend.position = "none"
     ,plot.title = element_text(hjust = 0.5)
    ) 

# Actual action for context 13 to select (get x value)
infoA <- ggplot_build(density13)$data[[3]] ## these are the actual action points
infoC <- ggplot_build(density13)$data[[2]] ## this is the density curve

c13h.xmax <- infoA$x[4] ## chose a high abnormality actual action
c13h.info <- infoC %>% filter(x < c13h.xmax)

c13.h <- density13 + 
         geom_segment(data=data.frame(), ##otherwise you draw this segment for every row of data...
                      aes(x = 30, y = .021, xend = 45, yend = .0025),
                  arrow = arrow(length = unit(0.45, "cm")),
                  color="red", alpha=.35, linetype=1) +
          geom_vline(xintercept=c13h.xmax,alpha=.5,linetype=3,color="red") +
          geom_area(data = c13h.info, aes(x=x, y=y), fill="red",alpha=.25) +
          annotate(geom="text", x=30, y=.022, label="Modal Distance(running away)",
            color="red",alpha=.65) 

#ggsave(c13.h,file="figs/modalDistance.png", width=8, height=6, units="in")
              
    


### Here where we calculate contextual modal distance for every action
modalDistTable <- data.frame()

for (c in unique(pgL$context)){
  
  numbSamples <- 10000
  
  pgSamp <- data.frame(sampId = seq(1,numbSamples))
  
  # samples 10,000 values from averages of participant responses for that context
  pgSamp$avg <- pgL %>%
    filter(context == c, question == 'avg') %>% 
    pull(response) %>% 
    sample(numbSamples, replace = T)  
  
  # samples 10,000 morality ratings from participant responses
  pgSamp$moral <- pgL %>%
    filter(context == c, question == 'Morality') %>% 
    pull(response) %>% 
    sample(numbSamples, replace = T)
  
  # samples 10,000 probability ratings from participant responses
  pgSamp$prob <- pgL %>%
    filter(context == c, question == 'Probability') %>% 
    pull(response) %>% 
    sample(numbSamples, replace = T)
  
  # samples 10,000 normality ratings from participant responses
  pgSamp$norm <- pgL %>%
    filter(context == c, question == 'Normality') %>% 
    pull(response) %>% 
    sample(numbSamples, replace = T) 
  
  
  # for each action, calculates the proportion of samples that are rated higher on average, higher by morality, higher by probability, and higher by normality than that action
  for (a in 1:6){
    modalDistTable <- evSum %>%
      filter(context == c) %>% 
      pivot_wider(names_from = question, values_from = mean) %>% 
      rename(mean = avg, moralityMean = Morality, probabilityMean = Probability, normalityMean = Normality) %>% 
      slice(a:a) %>%
      mutate(
        # proportion of samples that are rated higher on average than given action
        modalDistance = length(pgSamp$avg[mean<pgSamp$avg])/length(pgSamp$avg), 
          # proportion of samples that are rated higher by morality
              moralDistance = length(pgSamp$moral[moralityMean<pgSamp$moral])/length(pgSamp$moral), 
        # proportion of samples that are rated higher by morality
            probDistance = length(pgSamp$prob[probabilityMean<pgSamp$prob])/length(pgSamp$prob),
        # proportion of samples that are rated higher by normality
            normDistance = length(pgSamp$norm[normalityMean<pgSamp$norm])/length(pgSamp$norm)) %>%
      bind_rows(modalDistTable)
  }
  
}
```

## Study 3: Force judgments

### Participants
```{r participantsht, echo=F,warning=F,message=F}
# reads in participants' 'had to' judgments for each action
htW <- read.csv("data/hadTo.csv")%>% 
  rownames_to_column("id")
```

We collected a sample of `r length(unique(evW$id))` participants ($M_{age}$ = `r round(mean(htW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(htW$age,na.rm=T),2)`; `r table(htW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

### Procedure

Each participant was randomly presented with 12 of the 18 contexts. For each context, participants were randomly assigned one of the six actions and told that the agent decided to do that action. They then rated their agreement with a statement that the agent was forced to complete the action on a 100 point scale.

|     *Force statement:* [Agent] had to do [Action].

### Results

```{r tidyht, echo=F,warning=F,message=F}
# creates long table
htL <- htW %>% select(id, S1_1:S18_1) %>% 
  pivot_longer(-id, names_to = "context", values_to = "response", 
               values_drop_na = T) %>% 
  mutate(context = parse_number(context))

# adds information about which action each participant viewed for each context
htL <- htW %>% select(id, S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action",
               values_drop_na = T) %>% 
  mutate(context = parse_number(context)) %>% 
  filter(action != "") %>% 
  right_join(htL)  %>% 
  filter(context != 11) # removed context 11 because of a typo in the question

# creates summary table with average 'had to' rating for each action
htSum <- htL %>% group_by(context, action) %>% 
  summarise(hadTo = mean(response)) %>% 
  mutate(actionNumb = paste(context, as.integer(as.factor(action)), sep = "_")) %>% 
  ungroup()

# joins with modal distance table to correlate each action
htSum <- htSum %>% right_join(modalDistTable %>% select(-action)) %>% 
  relocate(hadTo, .after = modalDistance) %>% 
  filter(context != 11) 

# modal distance correlations with 'had to' judgments
htCorr <- htSum %>% with(list(cor.test(modalDistance, hadTo),
               cor.test(moralDistance, hadTo),
               cor.test(probDistance, hadTo),
               cor.test(normDistance, hadTo)))
```

Due to to a typo in the question for context 11 (the question referred to the wrong agent), results for this context were excluded from analysis.

Across all actions, participants reported a wide range of agreement with statements of force attribution ($M =$ `r htL %>% summarise(mean(response)) %>% pull() %>% round(1) %>% format(nsmall = 1)`, $SD =$ `r htL %>% summarise(sd(response)) %>% pull() %>% round(1) %>% format(nsmall = 1)`. As expected, using the modal distance value to predict average force judgments for each action, we found a correlation of $r =$ `r round(htCorr[[1]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(htCorr[[1]]$p.value[[1]], 3))`.\ 
The moral distance had a correlation of  $r =$ `r round(htCorr[[2]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(htCorr[[2]]$p.value[[1]], 3))` with force judgments; probability distance had a correlation of  $r =$ `r round(htCorr[[3]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(htCorr[[3]]$p.value[[1]], 3))` and normality distance had a correlation of  $r =$ `r round(htCorr[[4]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(htCorr[[4]]$p.value[[1]], 3))`.

```{r fig4, echo=F,warning=F,message=F}

fig4 <- htSum %>% mutate(context=factor(context)) %>%
                  ggplot(aes(y=hadTo,x=modalDistance)) +
                  geom_point(aes(color=context)) +
                  labs(y="Agreement agent was forced", 
                       x="Modal distance of actual action") +
                  geom_smooth(method=lm)+
                  theme_bw() +
                  theme(
                    plot.background = element_blank()
                    #,legend.position = "top"
                    ,panel.grid.major = element_blank()
                    ,panel.grid.minor = element_blank()
                    ,axis.text=element_text(size=rel(1.25))
                    #,axis.title=element_blank()
                    #,axis.title.y=element_blank()
                    #,axis.text.x=element_text(size=rel(1.5))
                    ,axis.title = element_text(size=rel(1.25))
                    ,axis.ticks = element_blank()
                  )

plot(fig4)


#ggsave(fig4,file="figs/fig4.png",width = 10,height = 8)

```

## Study 4: Causal judgments
For each context, we came up with one downstream consequence that could potentially occur regardless of which of the six actions the agent decided to. Downstream consequences can be found in Table 1 in the Appendix. We sought to predict participants' ratings of whether the action chosen by the agent caused these downstream consequences using our existing modal distance value. These judgments don't just reflect participant's judgments of the actual action but also the relationship between the action and downstream consequence. We also sought to compare our model to an existing model of causal judgments that involves judgments of whether a potential cause was sufficient for the outcome. We collected data in three studies, the first on causal judgments, the second on counterfactual relevance and necessity ratings of the action and the third on the sufficiency ratings.	

### Study 4a: Causal judgments

#### Participants

```{r participantscausal, echo=F,warning=F,message=F}
# reads in data for participants' ratings of whether the agent caused the downstream consequence
causalW <- read.csv("data/causal.csv") %>% 
  rownames_to_column("id")

# reads in data for participants' ratings of whether the action was necessary for causing the outcome and for the relevancy of alternative events
counterW <- read.csv("data/causal+counterfactual.csv") %>% 
  rownames_to_column("id") %>% 
  rename_with(~gsub("\\.", "_", .x), .cols = S1_1_1:S18.2_1)

# reads in data for participant ratings of whether the action was sufficient for the downstream consequence
suffW <- read.csv("data/sufficiencyCounterfactual.csv") %>% 
  rownames_to_column("id") %>% 
  rename_with(~gsub("\\.", "_", .x), .cols = S1_1_1:S18.1_1)

```

For the first study, we collected a sample of `r length(unique(causalW$id))` participants ($M_{age}$ = `r round(mean(causalW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(causalW$age,na.rm=T),2)`; `r table(causalW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

#### Procedure

Each participant viewed a randomized subset of 12 of the 18 contexts. For each context, participants read the story and were told that the agent decided to do one of the six actions. They then read that afterwards, another event happened. Participants were then asked to rate their agreement with the following statement on a 100 point scale:

|     *Causal statement:* [Agent] caused [Downstream consequence].

### Study 4b: Counterfactual judgments

#### Participants

For the second study, we collected a sample of `r length(unique(counterW$id))` participants ($M_{age}$ = `r round(mean(counterW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(counterW$age,na.rm=T),2)`; `r table(counterW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

#### Procedure

Participants viewed a randomized subset of 12 of the 18 contexts. For each context, participants, read the story and were told that the agent decided to do one of the six actions. They then read that afterwards, another event happened. Participants were then asked to rate their agreement with the following statements on 100 point scales:

|     *Counterfactual relevance statement:* Given the situation they were in, how relevant is it to consider the possibility that [Agent] could have done
|     something other than deciding to [Chosen action].

|     *Necessity statement:* If [Agent] hadn't decided to [Chosen action], [Downstream consequence] wouldn't have occurred.
| \ 
These statements were modified as necessary to make the sentence flow naturally (e.g. "If Heinz hadn't decided to [Chosen action], his wife wouldn't have gotten more ill.")

### Study 4c: Sufficiency judgments	

#### Participants	
For a third study, we collected a sample of `r length(unique(suffW$id))` participants ($M_{age}$ = `r round(mean(suffW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(suffW$age,na.rm=T),2)`; `r table(suffW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).	

#### Procedure	
Participants viewed a randomized subset of 12 of the 18 contexts. For each context, participants, read the story and were told that the agent decided to do one of the six actions. They then read that afterwards, another event happened. Participants were asked to rate their agreement with the following statement on a 100-point scale:

|     *Sufficiency statement:* Given that [Agent] decided to [Chosen action], [Downstream consequence] was going to happen.
| \ 
These statements were modified as necessary to make the sentence flow naturally (e.g. "Given that Heinz decided to [Chosen action], his wife was going to get more ill.")


### Results

```{r tidycausal, warning=F,message=F,echo=F}
# turns table to long
causalL <- causalW %>% filter(Progress == 100) %>% 
  select(id,S1_1:S18_1) %>% 
  pivot_longer(-id, names_to = "context", values_to = "response", values_drop_na = T) %>% 
  mutate(across(1:2, parse_number)) 

# Join with info about which action was being judged in each context
causalL <- causalW %>% filter(Progress == 100) %>% 
  select(id,S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action") %>% 
  filter(action != "") %>% 
  mutate(across(1:2, parse_number)) %>% 
  right_join(causalL)

# Read in the necessity judgments for each action/outcome pair
counterL <- counterW %>% filter(Progress == 100) %>% 
  select(id,S1_1_1:S18_2_1) %>% 
  pivot_longer(-id, names_to = "context_question", values_to = "response", values_drop_na = T) %>% 
  separate(context_question, into = c("context", "question", "null")) %>% 
  select(-null) %>% 
  mutate(across(1:2, parse_number), question = as.factor(question)) %>% 
  filter(id > 30) %>% # removes participants in pilot
  mutate(question = recode_factor(question, "1" = "counterfactual_relevance", "2" = "counterfactual_necessity"))

# Join with info about which action was being judged in each context
counterL <- counterW %>% filter(Progress == 100) %>% 
  select(id,S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action") %>% 
  filter(action != "") %>% 
  mutate(across(1:2, parse_number)) %>% 
  filter(id > 30) %>% # removes participants in pilot
  right_join(counterL)

# Read in the sufficiency judgments for each action/outcome pair
suffL <- suffW %>% filter(Progress == 100) %>% 
  select(id,S1_1_1:S18_1_1) %>% 
  pivot_longer(-id, names_to = "context", values_to = "response", values_drop_na = T) %>%
  filter(response != "") %>%
  mutate(across(1:2, parse_number))


# Join with info about which action was being judged in each context
suffL <- suffW %>% filter(Progress == 100) %>% 
  select(id,S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action") %>% 
  filter(action != "") %>% 
  mutate(across(1:2, parse_number)) %>% 
  right_join(suffL)



# bringing these all together at the level of each 6 action-outcome pairs for all 18 contexts
causalSum <- counterL %>% 
   group_by(context, action, question) %>% 
   summarise(mean = mean(response)) %>% 
   pivot_wider( names_from = question, values_from = mean) %>% 
   group_by(context) %>% 
  mutate(actionNumb = paste(context, row_number(), sep = "_"), .after = context) %>% 
  right_join(suffL %>% group_by(context,action) %>%
               summarise(suffCF = mean(response))
  ) %>%
  right_join(causalL %>% group_by(context, action) %>% 
                         summarise(causal = mean(response))
  ) %>% 
  right_join(modalDistTable %>% select(-c(action, mean))) %>% 
      # necessity strength = modalDistance (relevance of alternative events) * counterfactual_necessity (would the downstream consequence have occurred if the action hadn't happened?)
  mutate(necessity_strength = modalDistance * counterfactual_necessity)


##  Here, we attempt to get the probability of participants sampling an event in which E=a_a, or the probability of sampling the actual action done for each actual action.

# read in coding of each actual action into our manual coding groups
actualActions <- read.csv("materials/actualActions.csv", row.names = "X") 

causalSum  <- causalSum %>% arrange(context, actionNumb) %>%
  bind_cols(actualActions %>% arrange(context, actionNumb) %>% select(group))

causalSum <- causalSum %>% group_by(context, group)%>% 
  mutate(probSamp = 
                      if_else(group == 0, 0, # if an actual action was not in coded as part of an action group, it has a probability of sampling of 0
                              nrow(coded[coded$context == context & coded$group == group,])/nrow(coded[coded$context == context,])) # otherwise, actions are given a value equal to the proportion of responses for the context in the same action category
         ) %>% 
  ungroup() %>% 
  mutate(suff_strength = probSamp * suffCF, # multiply probability of sampling an action in the same category by participant ratings that the action was sufficient for the downstream consequence.
         icard_model = necessity_strength + suff_strength, # This is the Icard model causal term
        icard_modified = necessity_strength + suffCF)

potencyCorr <- with(causalSum, cor.test(necessity_strength, causal))
causalCorr <- with(causalSum, list(cor.test(moralDistance, causal),
                     cor.test(probDistance, causal),
                     cor.test(normDistance, causal)))
modelCorr <- with(causalSum, cor.test(suff_strength,necessity_strength))
suffCorr <- with(causalSum, cor.test(suff_strength,causal))


# full model
lmer0 <- lmer(data = causalSum,
              causal ~ counterfactual_necessity * modalDistance + (1|context))
# model with interaction removed
lmer1 <- lmer(data = causalSum,
     causal ~ counterfactual_necessity + modalDistance + (1|context))
# model with counterfactual_necessity removed
lmer2 <- lmer(data = causalSum,
              causal ~ modalDistance + (1|context))
# model with modalDistance removed
lmer3 <- lmer(data = causalSum,
              causal ~ counterfactual_necessity + (1|context))

causeAnovas <- list(
  # check that interaction between two terms is better than treating them as separate variables
  anova(lmer0, lmer1),
  # check that model without counterfactual_necessity does worse
  anova(lmer1, lmer2),
  # check that model without modal distance does worse
  anova(lmer1, lmer3))

# demonstrate that counterfactual relevance is a modal background representation
relevanceCorr <- with(causalSum, cor.test(modalDistance, counterfactual_relevance))



# Here we compare our model to Icard's model that includes sufficiency judgments

# model with both predictors
lmer0 <- lmer(data = causalSum, causal  ~ necessity_strength + icard_model + (1|context))

# model with our predictor removed
lmer1 <- lmer(data = causalSum, causal  ~  icard_model + (1|context))	

# model with the alternative predictor removed 
lmer2 <- lmer(data = causalSum, causal  ~ necessity_strength + (1|context))	

# model including components of both our and Icard's models
lmer3 <- lmer(data = causalSum, causal  ~ counterfactual_necessity + modalDistance + suffCF + (1|context))	

# model with sufficiency removed
lmer4 <- lmer(data = causalSum, causal  ~ counterfactual_necessity + modalDistance + (1|context))	

suffAnovas <- list(
  # check that our necessity strength predictor is significant
  anova(lmer0,lmer1),
  # check whether the Icard measure is significant
  anova(lmer0,lmer2), 
  # check whether sufficiency adds anything when modal distance and counterfactual_necessity are included
  anova(lmer3, lmer4))	

```
Participants reported a wide range of agreement with statements attributing causal responsibility to the agents ($M=$ `r causalL %>% summarise(mean(response)) %>% pull() %>% round(1)`, $SD=$ `r causalL %>% summarise(sd(response)) %>% pull() %>% round(1)`).\ 
To predict participants' judgments of causality for each event, we generated a new measure that incorporated the modal distance for each event and participants' judgments of the counterfactual relevance. The modal distance should only play a role in causal judgments if participants judge the event as necessary for bringing out the downstream consequence. As such, we multiplied participant ratings for counterfactual necessity with modal distance values generating a value which we're calling "necessity strength" for each event. As predicted, across action-context pairs, necessity strength was highly correlated with causal ratings ($r =$ `r round(potencyCorr$estimate[[1]], 3)`, $p <$ `r max(.001, round(potencyCorr$p.value, 3))`).\ 
The moral distance had a correlation of  $r =$ `r round(causalCorr[[1]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(causalCorr[[1]]$p.value[[1]], 3))` with force judgments; probability distance had a correlation of  $r =$ `r round(causalCorr[[2]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(causalCorr[[2]]$p.value[[1]], 3))` and normality distance had a correlation of  $r =$ `r round(causalCorr[[3]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(causalCorr[[3]]$p.value[[1]], 3))`. \ 
To ensure that both the modal distance and the necessity rating, as well as the interaction between them, were critical for predicting causal ratings, we next conducted a series of model comparisons. Specifically, we built a linear mixed-effects model predicting causal ratings as a function of modal distance, counterfactual ratings and their interaction, with a random effect for context. The full model performed significantly better than a model in which contextual modal distance was removed ($\chi^2$(`r causeAnovas[[3]]$Df[[2]]`) = `r round(causeAnovas[[3]]$Chisq[[2]], 3)`, $p <$ `r max(.001, round( causeAnovas[[3]]$"Pr(>Chisq)"[[2]], 3))`), better than a model in which counterfactual necessity ratings were removed ($\chi^2$(`r causeAnovas[[2]]$Df[[2]]`) = `r round(causeAnovas[[2]]$Chisq[[2]],3)`, $p <$ `r max(.001, round( causeAnovas[[2]]$"Pr(>Chisq)"[[2]], 3))`), and critically, also better than a model in which the interaction between the two was removed ($\chi^2$(`r causeAnovas[[1]]$Df[[2]]`) = `r round(causeAnovas[[1]]$Chisq[[2]],3)`, $p <$ `r max(.001, round(causeAnovas[[1]]$"Pr(>Chisq)"[[2]], 3))`). These model comparisons confirm the importance of the relationship between modal distance and necessity: agents are most strongly judged to be causes of outcomes when it is both the case that there were relevant alternative actions that they could have done instead, and if they had done those actions instead, the outcome would likely have been different. \

We additionally found evidence that the counterfactual relevance measure used in prior work reflects modal distance: The contextual modal distance of each actual action was highly correlated with participants' explicit judgments of the relevance of counterfactual alternatives, $r =$ `r round(relevanceCorr$estimate, 3)`, $p <$ `r max(.001, round(relevanceCorr$p.value, 3))`.\	

We also constructed a series of linear mixed effects models to compare our model against the existing model of causal judgments. This model includes a term for sufficiency strength, which we calculated by multiplying the probability of sampling an action by participants' judgments of the sufficiency of the action for causing the downstrema consequence. First we constructed a linear mixed-effects model predicting causal judgments using necessity strength (our measure) and the Icard model's measure. The model performed significantly worse with our measure removed ($\chi^2$(`r suffAnovas[[1]]$Df[[2]]`) = `r round(suffAnovas[[1]]$Chisq[[2]],3)`, $p <$ `r max(.001, round(suffAnovas[[1]]$"Pr(>Chisq)"[[2]], 3))`) but not when the Icard measure was removed ($\chi^2$(`r suffAnovas[[2]]$Df[[2]]`) = `r round(suffAnovas[[2]]$Chisq[[2]],3)`, $p =$ `r max(.001, round(suffAnovas[[2]]$"Pr(>Chisq)"[[2]], 3))`). Because the primary difference between the two measures is that the Icard model includes a term for sufficiency judgments, we constructed another model to see whether sufficiency judgments of the actions contributed anything to causal judgments. We constructed a mixed-effects model predicting causal judgments with necessity strength and sufficiency and a random effect for context. Removing the sufficiency ratings did not significantly effect performance of the model ($\chi^2$(`r suffAnovas[[3]]$Df[[2]]`) = `r round(suffAnovas[[3]]$Chisq[[2]],3)`, $p =$ `r max(.001, round(suffAnovas[[3]]$"Pr(>Chisq)"[[2]], 3))`). \ 


```{r fig5, echo=F,warning=F,message=F}
figS4 <- causalSum %>% mutate(context=factor(context)) %>%
                  ggplot(aes(y=modalDistance,x=counterfactual_relevance)) +
                  geom_point(aes(color=context)) +
                  labs(y="Modal distance of the actual action", 
                       x="Counterfactual relevance score") +
                  geom_smooth(method=lm)+
                  theme_bw() +
                  theme(
                    plot.background = element_blank()
                    #,legend.position = "top"
                    ,panel.grid.major = element_blank()
                    ,panel.grid.minor = element_blank()
                    ,axis.text=element_text(size=rel(1.25))
                    #,axis.title=element_blank()
                    #,axis.title.y=element_blank()
                    #,axis.text.x=element_text(size=rel(1.5))
                    ,axis.title = element_text(size=rel(1.25))
                    ,axis.ticks = element_blank()
                  )


ggsave(figS4,file="figs/figS4.jpg", width = 10, height = 8)


g <- ggplot_build(figS4)
colors <- unique(g$data[[1]]$colour)
colors <- colors[c(1,13,15)]




## graph of just the necessity ratings for the presentation

necessity3 <-  causalSum %>% 
  filter(context %in% c(1,13,15)) %>%
  ggplot(aes(x = reorder(action, modalDistance), 
             y = counterfactual_necessity, 
             label = action, fill = modalDistance)) +
  labs(x = "Actual Action", y = "Counterfactual Necessity", fill = "Modal Distance") +
  geom_col() +
  facet_wrap(~context,scales = "free_x") +
  theme_bw() +
  theme(
    plot.background = element_blank()
    #,legend.position = "none"
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
    ,axis.text.y=element_text(size=rel(1.4))
    #,axis.title=element_blank()
    #,axis.title.y=element_blank()
    ,axis.text.x=element_text(size=rel(1.5), angle = 60, hjust = 1)
    ,axis.title.x = element_text(size=rel(2))
    ,axis.ticks = element_blank()
  )

ggsave(necessity3,file="figs/necessity3.jpg",width = 24,height = 8)


potency3 <-  causalSum %>% 
  filter(context %in% c(1,13,15)) %>%
  ggplot(aes(x = reorder(action, modalDistance), 
             y = counterfactual_c, 
             label = action, fill = modalDistance)) +
  labs(x = "Actual Action", y = "Necessity Strength", fill = "Modal Distance") +
  geom_col() +
  facet_wrap(~context,scales = "free_x") +
  theme_bw() +
  theme(
    plot.background = element_blank()
    #,legend.position = "none"
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
    ,axis.text.y=element_text(size=rel(1.4))
    #,axis.title=element_blank()
    #,axis.title.y=element_blank()
    ,axis.text.x=element_text(size=rel(1.5), angle = 60, hjust = 1)
    ,axis.title.x = element_text(size=rel(2))
    ,axis.ticks = element_blank()
  )

```

## Study 5: Blame judgments

Using the same stimulus sets from Study 4, we sought to determine whether the modal distance could further be used to predict participants' moral judgments of the agents. Specifically, we investigated whether participants judged that the agent should be blamed for the downstream consequence occurring.

### Participants

```{r participantsblame, echo=F, warning=F,message=F}
# reads in data for whether ratings of agent should be blamed for downstream consequence
blameW <- read.csv("data/blame.csv") %>% 
  rownames_to_column("id")
```

We collected a sample of `r length(unique(blameW$id))` participants ($M_{age}$ = `r round(mean(blameW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(blameW$age,na.rm=T),2)`; `r table(blameW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

### Procedure

Study design was identical to Study 4a, except that instead of being asked causal questions, they were instead asked to rate their agreement with a statement of blame attribution on a 100-point scale:

|     *Blame statement:* [Agent] should be blamed for [Downstream consequence].

### Results

```{r tidyBlame, echo=F,warning=F,message=F}
# pivots table to long format
blameL <- blameW %>% 
  filter(Progress ==100) %>% 
  select(id, S1_1:S18_1) %>% 
  pivot_longer(-id, names_to = "context", values_to = "response",
               values_drop_na = T) %>% 
  mutate(across(1:2, parse_number))

# joins with which actions each participant viewed for each context
blameL <- blameW  %>%
  filter(Progress ==100) %>% 
  select(id, S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action") %>% 
  filter(action != "") %>% 
  mutate(across(1:2, parse_number)) %>% 
  right_join(blameL, by = c("id", "context"))

# creates summary table with average blame ratings for each action
blameSum <- blameL %>% 
  group_by(context, action) %>% 
  summarise(blame = mean(response), .groups = "drop") %>% 
  group_by(context) %>% 
  mutate(actionNumb = paste(context, row_number(), sep = "_"), .after = context)

# adds modal distance values
blameSum <- blameSum %>%
  full_join(causalSum %>% select(context, actionNumb, counterfactual_necessity, necessity_strength, modalDistance, moralDistance, probDistance, normDistance),
                       by = c("context", "actionNumb"))

blameCorr <- with(blameSum,cor.test(necessity_strength, blame))
blameCorrs <- with(blameSum, list(cor.test(moralDistance, blame),
                     cor.test(probDistance, blame),
                     cor.test(normDistance, blame)))

# full model
lmer0 <- lmer(data = blameSum,
              blame ~ counterfactual_necessity * modalDistance + (1|context))
# model with interaction removed
lmer1 <- lmer(data = blameSum,
     blame ~ counterfactual_necessity + modalDistance + (1|context))
# model with counterfactual_necessity removed
lmer2 <- lmer(data = blameSum,
              blame ~ modalDistance + (1|context))
# model with modalDistance removed
lmer3 <- lmer(data = blameSum,
              blame ~ counterfactual_necessity + (1|context))

blameAnovas <- list(
  # check that interaction between two terms is better than treating them as separate variables
  anova(lmer0, lmer1),
  # check that model without counterfactual_necessity does worse
  anova(lmer1, lmer2),
  # check that model without modal distance does worse
  anova(lmer1, lmer3))

```

Once again, participants reported a wide range of agreement with statements attributing blame to the agent ($M=$ `r blameL %>% summarise(mean(response)) %>% pull() %>% round(1)`, $SD=$ `r blameL %>% summarise(sd(response)) %>% pull() %>% round(1)`).	\
As with causal judgments, necessity strength was highly correlated with attributions of blame to the agent who acted ($r =$ `r round(blameCorr$estimate[[1]], 3)`, $p <$ `r max(.001, round(potencyCorr$p.value, 3))`). Additionally, we used the same linear mixed effects model we previously used to predict causal judgments to predict attribution of blame. We again selectively lesioned the full model to create three new models: one with modal distance removed, one with counterfactual necessity removed, and one with the interaction term between the two removed. All three models performed significantly worse than the full model (modal distance removed: $\chi^2$(`r blameAnovas[[3]]$Df[[2]]`) = `r round(blameAnovas[[3]]$Chisq[[2]], 3)`, $p <$ `r max(.001, round( blameAnovas[[3]]$"Pr(>Chisq)"[[2]], 3))`; counterfactual necessity removed: $\chi^2$(`r blameAnovas[[2]]$Df[[2]]`) = `r round(blameAnovas[[2]]$Chisq[[2]], 3)`, $p <$ `r max(.001, round( blameAnovas[[2]]$"Pr(>Chisq)"[[2]], 3))`; interaction removed: $\chi^2$(`r blameAnovas[[1]]$Df[[2]]`) = `r round(blameAnovas[[1]]$Chisq[[2]], 3)`, $p <$ `r max(.001, round(blameAnovas[[1]]$"Pr(>Chisq)"[[2]], 3))`). These model comparisons again confirm the importance of the relationship between modal distance and necessity: agents are most held responsible when it is both the case that there were better options available and if they had chosen those alternative options instead, the outcome would have likely been different. \ 
The moral distance had a correlation of  $r =$ `r round(blameCorrs[[1]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(blameCorrs[[1]]$p.value[[1]], 3))` with force judgments; probability distance had a correlation of  $r =$ `r round(blameCorrs[[2]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(blameCorrs[[2]]$p.value[[1]], 3))` and normality distance had a correlation of  $r =$ `r round(blameCorrs[[3]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(blameCorrs[[3]]$p.value[[1]], 3))`.

```{r figS4, echo=F,warning=F,message=F}

figS4_top <- blameSum %>% mutate(context=factor(context)) %>%
                  ggplot(aes(y=blame,x=necessity_strength)) +
                  geom_point(aes(color=context)) +
                  labs(y="Agreement that the agent should be blamed", 
                       x="Necessity strength of the actual action") +
                  geom_smooth(method=lm)+
                  theme_bw() +
                  theme(
                    plot.background = element_blank()
                    #,legend.position = "top"
                    ,panel.grid.major = element_blank()
                    ,panel.grid.minor = element_blank()
                    ,axis.text=element_text(size=rel(1.25))
                    #,axis.title=element_blank()
                    #,axis.title.y=element_blank()
                    #,axis.text.x=element_text(size=rel(1.5))
                    ,axis.title = element_text(size=rel(1.25))
                    ,axis.ticks = element_blank()
                  )

g <- ggplot_build(figS4_top)
colors <- unique(g$data[[1]]$colour)
colors <- colors[c(1,13,15)]


figS4_bottom <-  blameSum %>% mutate(context=factor(context)) %>%
  filter(context %in% c("1","13","15")) %>%
  mutate(context = recode_factor(context, "1" = "Context 1", "13" = "Context 13", "15" = "Context 15"),
        context=factor(context,levels=c("Context 1","Context 13","Context 15"))
    ) %>%
  ggplot(aes(y=blame,x=necessity_strength)) +
        geom_point(aes(color=context)) +
        labs(y="Agreement that the agent should be blamed", 
             x="Necessity strength of the actual action") +
        facet_grid(~context) +
        scale_color_manual(values=colors) +
        geom_smooth(method=lm)+
        theme_bw() +
        theme(
          plot.background = element_blank()
          ,legend.position = "none"
          ,panel.grid.major = element_blank()
          ,panel.grid.minor = element_blank()
          ,axis.text=element_text(size=rel(1.25))
          #,axis.title=element_blank()
          #,axis.title.y=element_blank()
          #,axis.text.x=element_text(size=rel(1.5))
          ,axis.title = element_text(size=rel(1.25))
          ,axis.ticks = element_blank()
          ,strip.text = element_text(size=rel(1.25))
        )
plot(figS4_top)
plot(figS4_bottom)


ggsave(figS4_top,file="figs/figS4_top.jpg",width = 12,height = 10)
ggsave(figS4_bottom,file="figs/figS4_bottom.jpg",width = 12,height = 4)

```

## Study 6: Shifted modal space

Here we seek to demonstrate that these judgments are context-relative. This addresses a potential alternative explanation that we're simply capturing judgments of how normal each action is (regardless of context) and using that to predict the various high level judgments. To rule out this explanation, we constructed a new set of 18 'altered' contexts by taking the 18 previous contexts and changing 1-2 details in each. These changes were designed to result in shifts in the modal spaces, while keeping each of the six actions for each context feasible. These altered contexts can be found in Table 3 in the appendix. We intentionally set up the changes so that some would result in a more constrained modal space, some would result in a less constrained modal space, and some would result in other shifts. However, we weren't interested predicting how each change in the text would shift the modal space but rather hoped to determine whether, given a shift in the modal space, this difference would predict any difference in high-level judgments of the actions. We decided that demonstrating that this held for judgments of force would be sufficient. In order to test this, we had to replicate Studies 1-3 with the new altered contexts. Study 6a has the same design as Study 1, Study 6b has the same design as Study 2 and Study 6c has the same design as Study 3.

```{r altparticipants, echo=F, message=F, warning=F}
# reads in possibility generation data for altered contexts
altpgW <- read.csv("data/altpg.csv") %>%  rownames_to_column(var = "id") %>% 
  mutate(id = as.numeric(id)) %>% filter(Progress==100)
## issue with participant who said their age was 358
altpgW$age[altpgW$age==358] <- NA

# reads in action ratings for altered contexts
altevW <- read.csv("data/altev.csv") %>% rownames_to_column(var = "id") %>% filter(Progress==100)

# reads in 'had to' ratings for altered contexts
althtW <- read.csv("data/althadTo.csv") %>% rownames_to_column(var = "id") %>% filter(Progress==100)


```

### Study 6a: Altered context possibility generation

#### Participants

We collected a sample of `r length(unique(altpgW$id))` participants ($M_{age}$ = `r round(mean(altpgW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(altpgW$age,na.rm=T),2)`; `r table(altpgW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

#### Procedure

Unlike in Study 1, which was conducted across three surveys, all 18 of the altered contexts were placed into one survey. Participants were randomly assigned 6 of the contexts. Otherwise, study design was identical to Study 1, with participants generating and then rating five possible actions per context.

### Study 6b: Altered context action ratings

#### Participants

We collected a sample of `r length(unique(altevW$id))` participants ($M_{age}$ = `r round(mean(altevW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(altevW$age,na.rm=T),2)`; `r table(altevW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

#### Procedure

Using the altered contexts instead of the original, study design was otherwise identical to Study 2, with participants rating one action per scenario on scales for probability, morality and normality

### Study 6c: Altered context force judgments

#### Participants

We collected a sample of `r length(unique(althtW$id))` participants ($M_{age}$ = `r round(mean(althtW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(althtW$age,na.rm=T),2)`; `r table(althtW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

#### Procedure

Using the altered contexts instead of the original, study design was otherwise identical to Study 3, with participants rating whether the agent 'had to' complete a particular action.

### Replication

```{r tidyaltreplication, echo=F,warning=F,message=F}
# creates long table with one response per row
altpgL <- altpgW %>% filter(Progress==100) %>% 
  select(id, S1A1_4:S18A5_6, -c(demo:ses, contains("FL"), contains("."))) %>% 
  pivot_longer(-id, names_to = "question", values_to = "response",
               values_drop_na = T) %>% 
  separate(question, into = c("context", "answer"), sep = "A") %>% 
  separate(answer, into = c("answer", "question"), sep = "_") %>% 
  mutate(across(2:4, parse_number))  %>% 
  mutate(question = if_else(question %in% c(4, 7), "Probability",
                        if_else(question %in% c(5, 8), "Morality", "Normality")))

# calculates average of morality, normality and probability ratings for each action generated by a participant
altpgL <- altpgL %>% pivot_wider(names_from = question, values_from = response ) %>%
                mutate(avg=(Normality+Probability+Morality)/3) %>%
                pivot_longer(cols=c(Normality,Morality,Probability,avg),
                             names_to = "question",values_to ="response")

# creates long table with action ratings
altevL <- altevW %>% filter(Progress==100) %>% 
  select(id, S1_4:S18_6) %>% 
  pivot_longer(-id, names_to = "context_question", values_to = "response",
               values_drop_na = T) %>% 
  separate(context_question, into = c("context", "question"), sep = "_") %>% 
  mutate(across(1:3, parse_number)) %>% 
  mutate(question = if_else(question == 4, "Probability", 
                            if_else(question == 5, "Morality", "Normality")))

# adds which action each participant viewed
altevL <- altevW %>% select(id, S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action") %>% 
  mutate(context = parse_number(context),
         id = as.numeric(id)) %>% 
  left_join(altevL, .) %>%
  relocate(action, .after = "context") 

# creates summary table with average ratings
altevSum <- altevL %>% 
  group_by(context, action, question) %>%
    summarise(mean = mean(response), .groups = "drop") %>%
    as.data.frame() %>% 
    group_by(context) %>% 
    mutate(actionNumb = paste(context, as.integer(as.factor(action)), sep = "_") ) %>% 
  relocate(actionNumb, .after = "context") %>% 
  pivot_wider(names_from = question, values_from = mean ) %>%
                mutate(avg=(Normality+Probability+Morality)/3) %>%
                pivot_longer(cols=c(Normality,Morality,Probability,avg),
                             names_to = "question",values_to ="mean")
  

# percentage of time that a participant rates each answer for a given scenario highest. ties go to the first answer
perBest <- altpgL %>% filter(question=="avg") %>%
  group_by(id, context) %>% 
  mutate(rank = rank(-response, ties.method = "first")) %>% 
  group_by(answer) %>% 
  summarise(numBest = sum(rank == 1), .groups = "drop") %>% 
  ungroup() %>% 
  mutate(perBest = numBest/sum(numBest)) 


# spreads out data into 1 row per participant response with 3 ratings in separate columns
altpgPredict <- altpgL %>% pivot_wider(names_from = "question",
                             values_from = "response")


# Replicating result that as participants generate more answers, they rate them lower
if(file.exists("lmerOutputs/altpgPredictP.rda")){
  # saves computational time to just read these tables instead of recalculating them every time
  pgPredictN <- readRDS("lmerOutputs/altpgPredictN.rda")
  pgPredictP <- readRDS("lmerOutputs/altpgPredictP.rda")
  pgPredictM <- readRDS("lmerOutputs/altpgPredictM.rda")
} else{
  lmerSamp_Full <- lmer(as.numeric(answer) ~ scale(Morality) + scale(Normality) +  scale(Probability) + 
                       (1|context) + 
                       (scale(Morality) + scale(Normality) +  scale(Probability) |id),
                     data=altpgPredict)
  
  # lesion out probability
  lmerSamp_P <- lmer(as.numeric(answer) ~ scale(Morality) + scale(Normality) + 
                       (1|context) + 
                       (scale(Morality) + scale(Normality) +  scale(Probability) |id),
                     data=altpgPredict)
  # lesion out morality
  lmerSamp_M <- lmer(as.numeric(answer) ~ scale(Probability) + scale(Normality) + 
                       (1|context) + 
                       (scale(Morality) + scale(Normality) +  scale(Probability) |id),
                     data=altpgPredict)
  # lesion out normality
    lmerSamp_N <- lmer(as.numeric(answer) ~ scale(Morality) + scale(Probability) + 
                       (1|context) + 
                       (scale(Morality) + scale(Normality) +  scale(Probability) |id),
                     data=altpgPredict)
    
  pgPredictP <- anova(lmerSamp_Full,lmerSamp_P)
      saveRDS(pgPredictP, file = "lmerOutputs/altpgPredictP.rda")
  pgPredictM <- anova(lmerSamp_Full,lmerSamp_M)
      saveRDS(pgPredictM, file = "lmerOutputs/altpgPredictM.rda")
  pgPredictN <- anova(lmerSamp_Full,lmerSamp_N)
    saveRDS(pgPredictN, file = "lmerOutputs/altpgPredictN.rda")
  
}


#  anova predicting answer number by average rating of an answer
altpgPredict <- summary(aov(lm(as.numeric(answer) ~ avg, data=altpgPredict)))

# Replicating result that normality ratings can be predicted by interaction of probability and morality
if(file.exists("lmerOutputs/altpgNormInt.rda")){
  # saves computational time to just read these tables instead of recalculating them every time
  pgNormInt <- readRDS("lmerOutputs/altpgNormInt.rda")
  pgNormP <- readRDS("lmerOutputs/altpgNormP.rda")
  pgNormM <- readRDS("lmerOutputs/altpgNormM.rda")
} else{
  lmerNorm_Full <- lmer(scale(Normality) ~ scale(Morality) * scale(Probability) + (1|context) +
                   (scale(Probability) * scale(Morality) |id), data=altpgPredict)
  lmerNorm_Main <- lmer(scale(Normality) ~ scale(Morality) + scale(Probability) + (1|context) +
                   (scale(Probability) * scale(Morality) |id), data=altpgPredict)
  lmerNorm_P <- lmer(scale(Normality) ~ scale(Morality) + (1|context) +
                   (scale(Probability) * scale(Morality) |id), data=altpgPredict)
  lmerNorm_M <- lmer(scale(Normality) ~ scale(Probability) + (1|context) +
                   (scale(Probability) * scale(Morality) |id), data=altpgPredict)
  
  pgNormInt <- anova(lmerNorm_Full,lmerNorm_Main)
    saveRDS(pgNormInt, file = "lmerOutputs/altpgNormInt.rda") # these just save the models so you don't have to rerun later
  pgNormP <- anova(lmerNorm_Main,lmerNorm_P)
    saveRDS(pgNormP, file = "lmerOutputs/altpgNormP.rda")
  pgNormM <- anova(lmerNorm_Main,lmerNorm_M)
    saveRDS(pgNormM, file = "lmerOutputs/altpgNormM.rda")
  
  
 
}

```

We first replicated our analyses using data from the new contexts. Participants' first responses for each of the new contexts tended to be the one they rated highest, or tied for highest (`r round(perBest[[1,3]] * 100, 1)`% of the time).\

Once again, the better an option is, the earlier it tended to be generated by a participant ($F$(`r altpgPredict[[1]][["Df"]][1]`$) =$ `r round(altpgPredict[[1]][["F value"]][1],2)`, $p <$ `r max(.001, round(altpgPredict[[1]][["Pr(>F)"]][1],3))`). Further, independent of normality and morality, the more probable a response was, the more likely it was to be generated earlier ($\chi^2$(`r pgPredictP$Df[[2]]`) = `r round(pgPredictP$Chisq[[2]], 3)`, $p <$ `r max(.001, round(pgPredictP$"Pr(>Chisq)"[[2]], 3))`). The same is true for morality, where independent of the other ratings, the more moral a possibility was, the more likely it was to be generated earlier ($\chi^2$(`r pgPredictM$Df[[2]]`) = `r round(pgPredictM$Chisq[[2]], 3)`, $p <$ `r max(.001, round(pgPredictM$"Pr(>Chisq)"[[2]], 3))`). Once again, normality was not found to have a similar independent predictive effect, ($\chi^2$(`r pgPredictN$Df[[2]]`) = `r round(pgPredictN$Chisq[[2]], 3)`, $p =$ `r max(.001, round(pgPredictN$"Pr(>Chisq)"[[2]], 3))`).\

We then constructed a model identical to the one used earlier to predict normality ratings for possibilities generated for the new contexts. using morality and probability ratings. Once again, the model performed significantly worse when morality and probability were removed. ($\chi^2$(`r pgNormInt$Df[[2]]`) = `r round(pgNormInt$Chisq[[2]], 3)`, $p <$ `r max(.001, round(pgNormInt$"Pr(>Chisq)"[[2]], 3))`). We further found that both ratings were independently predictive of normality. The model performed worse when probability was removed ($\chi^2$(`r pgNormP$Df[[2]]`) = `r round(pgNormP$Chisq[[2]], 3)`, $p <$ `r max(.001, round(pgNormP$"Pr(>Chisq)"[[2]], 3))`), and when morality was removed ($\chi^2$(`r pgNormM$Df[[2]]`) = `r round(pgNormM$Chisq[[2]], 3)`, $p <$ `r max(.001, round(pgNormM$"Pr(>Chisq)"[[2]], 3))`).

### Results
```{r tidy altresults, message=F,echo=F,warning=F}
### Recalculating modal distance for each action based on the answers participants generated for the altered contexts
altmodalDistTable <- data.frame()

for (c in unique(altpgL$context)){
  # samples 10,000 values from averages of participant judgments for that context
  pgSamp <- altpgL %>%
    filter(context == c, question == 'avg') %>% 
    pull(response) %>% 
    sample(10000, replace = T)
  
  # for each action, calculates the proportion of samples that are rated as more normal than that action
  for (a in 1:6){
    altmodalDistTable <- altevSum %>%
      filter(context == c, question == 'avg') %>% 
      slice(a:a) %>%
      mutate(altmodalDistance = length(pgSamp[mean<pgSamp])/length(pgSamp)) %>%
      bind_rows(altmodalDistTable)
  }
}

altmodalDistTable <- altmodalDistTable %>% select(-question)

# creates long table with ratings of whether participants had to do each action
althtL <- althtW %>% select(id, S1_1:S18_1) %>% 
  pivot_longer(-id, names_to = "context", values_to = "response", 
               values_drop_na = T) %>% 
  mutate(context = parse_number(context))

# joins with information on which action they viewed
althtL <- althtW %>% select(id, S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action",
               values_drop_na = T) %>% 
  mutate(context = parse_number(context)) %>% 
  filter(action != "") %>% 
  right_join(althtL) 

# creates summary table with average ratings for each context
althtSum <- althtL %>% group_by(context, action) %>% 
  summarise(hadTo = mean(response)) %>% 
  mutate(actionNumb = paste(context, as.integer(as.factor(action)), sep = "_")) %>% 
  ungroup()

# adds altered context modal distance values
althtSum <- althtSum %>% select(context, action, actionNumb, hadTo) %>% 
  rename(althadTo = hadTo) %>% 
  full_join(altmodalDistTable %>% select(-c(action,mean)))



# correlation between had to judgments and modal distance for altered context
althtCorr <- althtSum %>% 
  with(cor.test(altmodalDistance, althadTo))

# joins original and altered context 'had to' ratings and modal distance values
altCompare <- althtSum %>% 
  full_join(htSum %>% select(-mean)) %>% 
  mutate(modalDiff = modalDistance - altmodalDistance, # compute difference between original context modal distance and altered context modal distance for each action
         htDiff = hadTo - althadTo) %>% 
  filter(context != 11) # once again removing context 11 due to a typo in the question from Study 3



# seeing whether modal distance values from original contexts are correlated with had to judgments from shifted contexts and vice versa
acrossCorrs <- list(altCompare %>% with(cor.test(modalDistance, althadTo)), altCompare %>% with(cor.test(altmodalDistance, hadTo)))

# full model predicting original context force judgments
lmer0 <- lmer(data=altCompare, hadTo ~ altmodalDistance * modalDistance + (1|context))
# model with interaction of modalDistance and altmodalDistance removed
lmer1 <- lmer(data=altCompare, hadTo ~ altmodalDistance + modalDistance + (1|context))
# original modal distance removed
lmer2 <- lmer(data=altCompare, hadTo ~ altmodalDistance + (1|context))
# shifted modal distance removed
lmer3 <- lmer(data=altCompare, hadTo ~ modalDistance + (1|context))


# full model predicting shifted context force judgments
lmer4 <- lmer(data=altCompare, althadTo ~ altmodalDistance * modalDistance + (1|context))
# model with interaction removed
lmer5 <- lmer(data=altCompare, althadTo ~ altmodalDistance + modalDistance + (1|context))
# original modal distance removed
lmer6 <- lmer(data=altCompare, althadTo ~ modalDistance + (1|context))
# shifted context model distance removed
lmer7 <- lmer(data=altCompare, althadTo ~ altmodalDistance + (1|context))


htModelComps <- list(
  # predicting original context had to judgments:
        anova(lmer0, lmer1), # effect of interaction
        anova(lmer1, lmer2), # effect of original modal distance
        anova(lmer1, lmer3), # effect of altered modal distance
  # predicting altered context had to judgments
        anova(lmer4, lmer5), # effect of interaction
        anova(lmer5, lmer6), # effect of altered modal distance
        anova(lmer5, lmer7)  # effect of original modal distance
        )

# correlation between difference and hadto judgments and difference in modal distance values for each action
diffCorr <- altCompare %>% with(cor.test(htDiff, modalDiff))
```


```{r fig6, message=F,echo=F,warning=F}
# Shifted modal distance figure

## Here is a fig trying to illustrate the shifting of modal space

contextChoice <- 13

altdensity13 <-  pgL %>% 
  filter(context == contextChoice) %>%
  filter(question == "avg") %>%
  mutate(abnomrality=100-response) %>%
    ggplot(aes(x = abnomrality)) +
      geom_density(lwd = 1, alpha = 0.25,fill="blue",color="blue") +
      geom_density(data= altpgL %>% filter(context == contextChoice) %>%  filter(question == "avg") %>%
                                 mutate(abnomrality=100-response), 
                   lwd = 1, alpha = 0.4, fill="darkred",color="darkred") +
      geom_label_repel(data = altevSum %>% filter(context == contextChoice) %>%
                               filter(question=="avg") %>%
                               mutate(abnormality=100-mean
                                      ,event=factor(action)
                                      ,shortevent = factor(c("blame M.O.H.",
                                                            "borrow ring",
                                                            "call taxi",
                                                            "run away",
                                                            "steal ring",
                                                            "tell sister")[event])), 
                 aes(x = abnormality, y = -0.0027, 
                     label = shortevent), 
                     seed=41,
                     direction = "y",
                     color="darkred",
                     alpha=.8,
                     size=rel(3.5),
                 segment.colour = NA
                ) +
    #xlab("Abnormality Score") +
    ylab("Density") +
    theme_bw() +
    theme(
      plot.background = element_blank()
      ,panel.grid.major = element_blank()
      ,panel.grid.minor = element_blank()
      ,legend.title=element_blank()
      ,legend.text=element_text(size=rel(1.25))
      ,axis.text.y=element_text(size=rel(1))
      ,axis.title.x=element_blank()
      ,axis.title.y=element_text(size=rel(.75))
      ,axis.ticks = element_blank()
      ,strip.text=element_text(size=rel(1))
      ,axis.title=element_text(size=rel(1))
     ,legend.position = "none"
     ,plot.title = element_text(hjust = 0.5)
    ) 

modal_spaces <- altdensity13 +
           geom_segment(data=data.frame(), ##otherwise you draw this segment for every row of data...
                      aes(x = 60, y = .009, xend = 50, yend = .0055),
                  arrow = arrow(length = unit(0.45, "cm")),
                  color="blue", alpha=.35, linetype=1) +
          annotate(geom="text", x=60, y=.0105, label="Original \n Modal Space",
            color="blue",alpha=.75)  +
        ## shifted modal distance
           geom_segment(data=data.frame(), ##otherwise you draw this segment for every row of data...
              aes(x = 30, y = .014, xend = 12.5, yend = .0125),
          arrow = arrow(length = unit(0.45, "cm")),
          color="darkred", alpha=.55, linetype=1) +
          annotate(geom="text", x=39, y=.014, label="Shifted \n Modal Space",
            color="darkred",alpha=.85) 

#ggsave(modal_spaces, file="figs/shiftedModalSpace.png",width=7, height=4, units="in")


## two illustrations of shifted modal distances:

#table(evSum$action[evSum$context==contextChoice])
actualAction1 <- "calling the taxi company"
actualAction2 <- "blaming it on the maid of honor"

actualAction <- actualAction2

altdensity_base <-  pgL %>% 
  filter(context == contextChoice) %>%
  filter(question == "avg") %>%
  mutate(abnomrality=100-response) %>%
    ggplot(aes(x = abnomrality)) +
      geom_density(lwd = 1, alpha = 0.25,color="blue") +
      geom_label(data = evSum %>% filter(context == contextChoice & action==actualAction) %>%
                                     filter(question=="avg") %>%
                                     mutate(abnormality=100-mean,
                                            action=factor(action),
                                            action=factor(c("Blame M.O.H.")[action])), 
                 aes(x = abnormality, y = -0.0005, 
                     label = action), 
                 color="blue",
                 alpha=.25,
                 size=rel(3),
                 segment.colour = NA
                ) +
      geom_density(data= altpgL %>% filter(context == contextChoice) %>%  filter(question == "avg") %>%
                                 mutate(abnomrality=100-response), 
                   lwd = 1, alpha = 0.5,color="darkred") +
      geom_label(data = altevSum %>% filter(context == contextChoice & action==actualAction) %>%
                                     filter(question=="avg") %>%
                                     mutate(abnormality=100-mean,
                                            action=factor(action),
                                            action=factor(c("Blame M.O.H.")[action])), 
                 aes(x = abnormality, y = -0.00125, 
                     label = action), 
                     color="darkred",
                     alpha=.35,
                     size=rel(3),
                 segment.colour = NA
                ) +
    xlab("Abnormality Score") +
    ylab("Density") +
    theme_bw() +
    theme(
      plot.background = element_blank()
      ,panel.grid.major = element_blank()
      ,panel.grid.minor = element_blank()
      ,legend.title=element_blank()
      ,legend.text=element_text(size=rel(1.25))
      ,axis.text.y=element_text(size=rel(1))
      ,axis.title.x=element_text(size=rel(1))
      ,axis.title.y=element_text(vjust=.9,size=rel(.75))
      ,axis.ticks = element_blank()
      ,strip.text=element_text(size=rel(1))
      ,axis.title=element_text(size=rel(1))
     ,legend.position = "none"
     ,plot.title = element_text(hjust = 0.5)
    ) 

# Extracting the modal distance curves from the graph -- original context
o.infoC <- ggplot_build(altdensity_base)$data[[1]] ## this is the density curve
o.infoA <- ggplot_build(altdensity_base)$data[[2]] ## this is the actual action point
o.c1h.xmax <- o.infoA$x[1] ##  get the abnormality score
o.c1h.info <- o.infoC %>% filter(x < o.c1h.xmax) ## get the modal distance curve

# Extracting the modal distance curves from the graph -- shifted context
s.infoC <- ggplot_build(altdensity_base)$data[[3]] ## this is the density curve
s.infoA <- ggplot_build(altdensity_base)$data[[4]] ## this is the actual action point
s.c1h.xmax <- s.infoA$x[1] ## get the abnormality score
s.c1h.info <- s.infoC %>% filter(x < s.c1h.xmax) # get the modal distance curve


## Blaming maid of honor
moh_shift <- altdensity_base +
      ## original modal distance
         geom_segment(data=data.frame(), ##otherwise you draw this segment for every row of data...
                      aes(x = 60, y = .009, xend = 50, yend = .0055),
                  arrow = arrow(length = unit(0.45, "cm")),
                  color="blue", alpha=.35, linetype=1) +
          geom_vline(xintercept=o.c1h.xmax,alpha=.5,linetype=3,color="blue") +
          geom_area(data = o.c1h.info, aes(x=x, y=y), fill="blue",alpha=.4) +
          annotate(geom="text", x=65, y=.010, label="Original \n Modal Distance",
            color="blue",alpha=.75)  +
        ## shifted modal distance
           geom_segment(data=data.frame(), ##otherwise you draw this segment for every row of data...
              aes(x = 30, y = .015, xend = 9.5, yend = .014),
          arrow = arrow(length = unit(0.45, "cm")),
          color="darkred", alpha=.35, linetype=1) +
          geom_vline(xintercept=s.c1h.xmax,alpha=.5,linetype=3,color="darkred") +
          geom_area(data = s.c1h.info, aes(x=x, y=y), fill="darkred",alpha=.4) +
          annotate(geom="text", x=35, y=.016, label="Shifted \n Modal Distance",
            color="darkred",alpha=.75)

ggsave(moh_shift, file="figs/mohiDistShift.png",width=4.5, height=6, units="in")


## alt modal distance figure
altmdist3 <-  altmodalDistTable %>% 
  filter(context %in% c(1,13,15)) %>%
  ggplot(aes(x = reorder(action, -mean), y = altmodalDistance, label = action,
             fill = mean)) +
  labs(x = "Actual Action", y = "Modal Distance", fill = "Normality Rating") +
  geom_col() +
  facet_wrap(~context,scales = "free") +
  theme_bw() +
  theme(
    plot.background = element_blank()
    #,legend.position = "none"
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
    ,axis.text.y=element_text(size=rel(1.4))
    #,axis.title=element_blank()
    #,axis.title.y=element_blank()
    ,axis.text.x=element_text(size=rel(1.5), angle = 60, hjust = 1)
    ,axis.title.x = element_text(size=rel(2))
    ,axis.ticks = element_blank()
  )

## replication figure
figN_top <- althtSum %>% mutate(context=factor(context)) %>%
                  ggplot(aes(y=althadTo,x=altmodalDistance)) +
                  geom_point(aes(color=context)) +
                  labs(y="Agreement agent was forced", 
                       x="Modal distance of actual action") +
                  geom_smooth(method=lm)+
                  theme_bw() +
                  theme(
                    plot.background = element_blank()
                    #,legend.position = "top"
                    ,panel.grid.major = element_blank()
                    ,panel.grid.minor = element_blank()
                    ,axis.text=element_text(size=rel(1.25))
                    #,axis.title=element_blank()
                    #,axis.title.y=element_blank()
                    #,axis.text.x=element_text(size=rel(1.5))
                    ,axis.title = element_text(size=rel(1.25))
                    ,axis.ticks = element_blank()
                  )

figM_top <- altCompare %>% mutate(context=factor(context)) %>%
                  ggplot(aes(y=htDiff,x=modalDiff)) +
                  geom_point(aes(color=context)) +
                  labs(y="Difference in agreement that the agent was forced", 
                       x="Difference in modal distance of actual action") +
                  geom_smooth(method=lm)+
                  theme_bw() +
                  theme(
                    plot.background = element_blank()
                    #,legend.position = "top"
                    ,panel.grid.major = element_blank()
                    ,panel.grid.minor = element_blank()
                    ,axis.text=element_text(size=rel(1.25))
                    #,axis.title=element_blank()
                    #,axis.title.y=element_blank()
                    #,axis.text.x=element_text(size=rel(1.5))
                    ,axis.title = element_text(size=rel(1.25))
                    ,axis.ticks = element_blank()
                  )

ggsave(figM_top,file="figs/figM_top.jpg",width = 12,height = 10)

```

We then sought do determine the relevance of context in judgments of whether agent was deserving of blame. \ 

First, we recalculated modal distance values for each of the actions, sampling participant responses from study 6a and using participant ratings of the actions from Study 6b. Replicating our previous result, we found a correlation of $r =$ `r round(althtCorr$estimate, 3)`, $p <$ `r max(.001, round(althtCorr$p.value, 3))` between the modal distance and the force judgments of actions.\ 

In order to demonstrate the context-specificity of force judgments, we calculated the correlation between original context modal distance values and altered context force judgments ($r =$ `r round(acrossCorrs[[1]]$estimate, 3)`, $p <$ `r max(.001, round(acrossCorrs[[1]]$p.value, 3))`) and between altered context modal distance values and new context force judgments ($r =$ `r round(acrossCorrs[[2]]$estimate, 3)`, $p <$ `r max(.001, round(acrossCorrs[[2]]$p.value, 3))`). These across-context correlations, were significant though not as predictive as within-context correlations, as predicted.\ 

We also constructed two models for predicting force judgments. We predicted force judgments for the altered contexts using both the original modal distance value and the new modal distance value for each action. We found no significant interaction between shifted and original modal distance ($\chi^2$(`r htModelComps[[4]]$Df[[2]]`= `r round(htModelComps[[4]]$Chisq[[2]], 3)`, $p =$ `r max(.001, round(htModelComps[[4]]$"Pr(>Chisq)"[[2]], 3))`). As expected, the model performed worse without the modal distance values from the altered contexts ($\chi^2$(`r htModelComps[[5]]$Df[[2]]`) = `r round(htModelComps[[5]]$Chisq[[2]], 3)`, $p <$ `r max(.001, round(htModelComps[[5]]$"Pr(>Chisq)"[[2]], 3))`). Interestingly, the model also performed somwehat worse without the original context modal distance values ($\chi^2$(`r htModelComps[[6]]$Df[[2]]`) = `r round(htModelComps[[6]]$Chisq[[2]], 3)`, $p =$ `r max(.001, round(htModelComps[[6]]$"Pr(>Chisq)"[[2]], 3))`).\ 
The second model predicted force judgments for the original contexts using the same terms. Once again, we found no significant interaction between shifted and original modal distance ($\chi^2$(`r htModelComps[[1]]$Df[[2]]`)= `r round(htModelComps[[1]]$Chisq[[2]], 3)`, $p =$ `r max(.001, round(htModelComps[[1]]$"Pr(>Chisq)"[[2]], 3))`). The model performed significantly worse when the original modal distance values were removed ($\chi^2$(`r htModelComps[[2]]$Df[[2]]`) = `r round(htModelComps[[2]]$Chisq[[2]], 3)`, $p <$ `r max(.001, round(htModelComps[[2]]$"Pr(>Chisq)"[[2]], 3))`), but was not effected by the removal of the altered context modal distance values ($\chi^2$(`r htModelComps[[3]]$Df[[2]]`) = `r round(htModelComps[[3]]$Chisq[[2]], 3)`, $p =$ `r max(.001, round(htModelComps[[3]]$"Pr(>Chisq)"[[2]], 3))`).\ 


We were further interested in determining if shifts in the modal space of a context would be reflected in shifts in force judgments of actions for that context. We calculated the difference between the original context modal distance value and the altered context modal distance value for each action. We also calculated the difference in force judgments of each action between the original and altered contexts. We found that differences in modal distance values for actions between contexts were significantly correlated with differences in force judgments between contexts ($r =$ `r round(diffCorr$estimate, 3)`, $p <$ `r max(.001, round(diffCorr$p.value, 3))`).

## Study 7: Within participant prediction

### Participants

```{r participantshtpg, echo=F,message=F,warning=F}
# reads in study 7 data including had to ratings and alternative action possibility generation
htpgW <- read.csv("data/htpg.csv") %>% 
  rownames_to_column(var = "id") %>% 
  filter(Progress==100)

```

We collected a sample of `r length(unique(htpgW$id))` participants ($M_{age}$ = `r round(mean(htpgW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(htpgW$age,na.rm=T),2)`; `r table(htpgW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)). 

### Procedure
Each participant was randomly assigned six of the 18 contexts and one of the six possible actions for that context. For each context participants completed three tasks. First, they rated their agreement with a 'had to' statement, as in Study 3. Second, they listed five alternative possibilities they believed they agent could have done. Finally, they rated each of their answers on 100-point scales for probability, morality and normality, as in Study 1.

### Results
```{r tidyhtpg, echo=F,warning=F,message=F}
htpgW <- htpgW %>% 
  # filter(Progress==100) %>% 
  select(id, S1_1:S18_A5_9, S1A:S18A)

# just the had to ratings
justht <- htpgW %>% 
  select(-contains("gen"),-contains("A")) %>% 
  pivot_longer(-id, names_to = "context", values_to = "hadTo",
               values_drop_na = T) %>% 
  mutate(across(1:2, parse_number))

# adds which actions participants were rating
justht <- htpgW %>% 
  select(id, S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action") %>% 
  filter(action != "") %>% 
  mutate(across(1:2, parse_number)) %>% 
  right_join(justht)

# just the alternative possibility generation ratings
justpg <- htpgW %>% select(id, contains("A") & where(is.numeric)) %>% 
  pivot_longer(-id, names_to = "context_answer_question", values_to = "response", values_drop_na = T) %>% 
  separate("context_answer_question", into = c("context", "answer", "question"), sep = "_") %>% 
  mutate(across(1:3, parse_number)) %>% 
  mutate(question = if_else(question %in% c(4, 7), "Probability",
                        if_else(question %in% c(5, 8), "Morality", "Normality"))) %>% 
  pivot_wider(names_from = "question", values_from = "response") %>% 
   mutate(avg = rowMeans(select(., Probability:Normality), na.rm = T)) %>%
  pivot_longer(cols=c(Normality,Morality,Probability,avg),
               names_to = "question",values_to ="response")

 

### Replicating the predictions of had to judgments using modal distance. across participants
acrossCorrData <- justht %>% group_by(context, action) %>% 
  summarise(hadTo = mean(hadTo)) %>% 
  group_by(context) %>% 
  mutate(actionNumb = paste(context, row_number(), sep = "_")) %>%
  right_join(modalDistTable %>% select(-c(action, moralityMean:probabilityMean, moralDistance:normDistance)))

acrossCorr <- acrossCorrData %>% with(cor.test(modalDistance, hadTo))


# alternative method for calculating modal distance that takes into account the quality of alternative actions, rather than just the proportion of actions that are better. This allows for a more fair comparison with within-participant sample distance, which takes does the same. modalSum is the sum of differences where v(actual action) < v(sampled action). 

acrossCorrData <- acrossCorrData %>% mutate(modalSum = -1) # adds dummy modalSum column
for (c in unique(acrossCorrData$context)){
  pgSamp <- justpg %>% 
    filter(context == c, question == 'avg') %>% 
    pull(response) %>% 
    sample(numbSamples, replace = T) # samples 10,000 average ratings per context
  
  for (a in 1:6){
    a_numb <- paste(c, a, sep = "_")
    a_val <- acrossCorrData %>% filter(actionNumb == a_numb) %>% pull(mean) # action value
    
    acrossCorrData <- acrossCorrData %>% mutate(modalSum = if_else(actionNumb == a_numb, sum(pgSamp[a_val < pgSamp]), modalSum)) # calculates sum of actions that are rated higher than a_val for each action
  }
  
}

# rescales modalSum so that all values are between 0 and 1
acrossCorrData <- acrossCorrData %>% ungroup() %>%  mutate(modalSum = (modalSum -min(modalSum))/ (max(modalSum) - min(modalSum))) 






## Calculating sample distance for each participant as proportion of actions where v(a_a) < v(a_s) and sample sum for each participant as the sum of values where v(a_a) < v(a_s)

withinCorrData <- justht %>% 
  left_join(acrossCorrData %>% select(action,context,actionNumb,modalDistance, modalSum), by=c("action","context")) %>% 
  mutate(sampleDistance = NA, sampleSum = NA) %>% 
  ungroup()

for (subj in unique(withinCorrData$id)){  ## for each participant

    d <- withinCorrData %>% filter(id==subj)
    
    ## for each context, over-sample the participant's generations
    for (c in unique(d$context)){ 
      
        pgSamp <- justpg %>% 
          filter(id==subj, context == c, question == 'avg') %>%
          pull(response)
      
        
        action_numb <- d$actionNumb[d$context==c]
        a_val <- evSum$mean[evSum$actionNumb==action_numb & evSum$question=="avg"]
        withinCorrData$sampleSum[withinCorrData$id==subj & withinCorrData$context==c] <- sum(pgSamp[a_val<pgSamp]) ## for each action, calculate the sum of values of actions that were rated higher than the average rating of the actual action
        withinCorrData$sampleDistance[withinCorrData$id==subj & withinCorrData$context==c] <- length(pgSamp[a_val<pgSamp])/length(pgSamp[]) ## for each actual action, calculate the proportion of the participant's own samples that were rated higher than the avg rating of the actual action
  
    }
}



# full model, including both sampleDistance and modalDistance
lmer1 <- lmer(data=withinCorrData, scale(hadTo) ~ scale(sampleDistance) + scale(modalDistance) +
         (scale(sampleDistance) + scale(modalDistance) |context) +
         (1|id))

# removing sampleDistance
lmer2 <- lmer(data=withinCorrData, scale(hadTo) ~ scale(modalDistance) + 
                (scale(sampleDistance) + scale(modalDistance)|context) +
                (1|id))

# removing modalDistance
lmer3 <-lmer(data=withinCorrData, scale(hadTo) ~ scale(sampleDistance) + 
               (scale(sampleDistance) + scale(modalDistance) |context) +
               (1|id))

# full model, including both sampleSum and modalSum
lmer4 <- lmer(data=withinCorrData, scale(hadTo) ~ scale(sampleSum) + scale(modalSum) +
         (scale(sampleSum) + scale(modalSum) |context) +
         (1|id))

# removing sampleSum
lmer5 <- lmer(data=withinCorrData, scale(hadTo) ~ scale(modalSum) + 
                (scale(sampleSum) + scale(modalSum)|context) +
                (1|id))

# removing modalSum
lmer6 <-lmer(data=withinCorrData, scale(hadTo) ~ scale(sampleSum) + 
               (scale(sampleSum) + scale(modalSum) |context) +
               (1|id))


withinCorr <- list(
  anova(lmer1, lmer3),anova(lmer1, lmer2), # comparing modal and sample distance measures 
  anova(lmer4, lmer6),anova(lmer4, lmer5), # comparing modal and sample sum measures 
  with(withinCorrData, cor.test(sampleDistance, hadTo)), with(withinCorrData, cor.test(sampleSum, hadTo)) # zero-order correlations between sample distance and sum with had to judgments
  )

## Here's a figure of the difference across the distance measures (not included in the paper but helpful)
fig7 <- withinCorrData %>% mutate(context=factor(context)) %>%
                          pivot_longer(c(modalDistance,sampleDistance),names_to = "distanceMeasure",values_to = "distance") %>%
                          mutate(distanceMeasure=factor(distanceMeasure),
                                 distanceMeasure=factor(c("Modal Distance","Online Sample")[distanceMeasure])) %>%
                ggplot(aes(y=hadTo,x=distance)) +
                  geom_point(aes(color=context)) +
                  labs(y="Agreement agent was forced", 
                       x="Distance Measure") +
                  geom_smooth(method=lm)+
                  facet_grid(~distanceMeasure, scales = "free_x") +
                  theme_bw() +
                  theme(
                    plot.background = element_blank()
                    #,legend.position = "top"
                    ,panel.grid.major = element_blank()
                    ,panel.grid.minor = element_blank()
                    ,axis.text=element_text(size=rel(1.25))
                    #,axis.title=element_blank()
                    #,axis.title.y=element_blank()
                    #,axis.text.x=element_text(size=rel(1.5))
                    ,axis.title = element_text(size=rel(1.25))
                    ,axis.ticks = element_blank()
                  )

```

As expected, the modal distance for each action predicted how likely participants were to judge agents as having been forced to do that action ($r =$ `r round(acrossCorr$estimate, 3)`, $p <$ `r max(.001, round(acrossCorr$p.value))`). We then sought to determine whether the specific alternative possibilities participants thought of could be used as better predictors than the across-participant modal space.
First, we calculated a 'sample distance' measure analogous to the modal distance measure. For the participant distance measure, for each action a participant viewed, we calculated the proportion of alternative actions they came up with for that context that they rated higher than the average value for the action. As expected, sample distance was negatively correlated with 'had to' judgments of actions ($r =$ `r round(withinCorr[[5]]$estimate, 3)`, $p<$ `r max(.001,round(withinCorr[[5]]$p.value))`). To investigate the extent to which sample distance explained variance unaccounted for by modal distance, we constructed a linear mixed effects model with modal distance and participant distance as fixed effects and a random effect for context. The model performed worse when modal distance was removed ($\chi^2$(`r withinCorr[[1]]$Df[[2]]`) $=$ `r round(withinCorr[[1]]$Chisq[[2]], 3)`, $p <$ `r max(.001,round(withinCorr[[1]]$"Pr(>Chisq)"[[2]],3))`) and when participant distance was removed ($\chi^2$(`r withinCorr[[2]]$Df[[2]]`) $=$ `r round(withinCorr[[2]]$Chisq[[2]], 3)`, $p =$ `r max(.001,round(withinCorr[[2]]$"Pr(>Chisq)"[[2]],3))`). \ 
We also sought to investigate the role of participant-generated alternatives using a different sampling method. For each context a participant viewed, we calculated the sum of average values of participant-generated alternatives that were rated higher than the average value of the actual action. Unlike the original modal distance method, which only takes into account the proportion of responses that are rated higher than a given action, this method takes into account how good these better alternatives are. If participants are comparing actions to specific generated alternatives, this method should be a strong predictor of their responses to the 'had to' question. However, in order to more fairly compare this method to the across-participant sampled method, which we are using as a proxy for a heuristic representation, we decided to recalculate the modal distance for each action in a way that similarly takes into account the value of options that are rated higher than the action. For each context, we once again sampled 10,000 values from participant generated responses. For each action, we then summed the values of all responses that were rated higher than the action. We then rescaled these results so all actions had a new modal distance value between 0 and 1. We will refer to the within-participant value as the 'sample sum' and the across-participant value as the 'modal sum.'\ 
Sample sum was negatively correlated with 'had to' judgments ($r =$ `r round(withinCorr[[6]]$estimate, 3)`, $p<$ `r max(.001,round(withinCorr[[6]]$p.value))`). Once again, we constructed a linear mixed effects model using the sample sum and modal sum as fixed effects and a random effect for context. The model performed significantly worse when modal sum was removed ($\chi^2$(`r withinCorr[[3]]$Df[[2]]`) $=$ `r round(withinCorr[[3]]$Chisq[[2]], 3)`, $p <$ `r max(.001,round(withinCorr[[3]]$"Pr(>Chisq)"[[2]],3))`), and when sample sum was removed ($\chi^2$(`r withinCorr[[4]]$Df[[2]]`) $=$ `r round(withinCorr[[4]]$Chisq[[2]], 3)`, $p =$ `r max(.001,round(withinCorr[[4]]$"Pr(>Chisq)"[[2]],3))`). However, note that sample sum accounted for a much smaller amount of the variance than modal sum. This suggests that while sampling specific alternative actions may play some role in high-level judgments, most of the work is likely being done by a heuristic representation.


## Appendix

### Table 1

```{r contextsTable, results='asis'}
table1 <- read.csv("materials/contextsTable.csv") %>% select(-alt_text)

# table1 <- table1 %>% rename_with(~sub("(.)", "\\U\\1",., perl=TRUE) %>% gsub('_', ' ',.))

kable(table1,booktabs=T)
```

### Table 2

```{r codingKey, results='asis'}
kable(codingKey)
```

### Table 3

```{r altcontextsTable, results='asis'}
table3 <- read.csv("materials/contextsTable.csv") %>% select(context, text, alt_text)

kable(table3,booktabs=T)
```
