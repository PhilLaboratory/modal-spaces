{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head\n",
    "Author: Eli Hecht\n",
    "Purpose: adapt Alina's sentence embedding code to group decision-study responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "from sentence_transformers import SentenceTransformer #load the model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtering for personal words\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "# import kaleido\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build directories at specified location\n",
    "def make_directories(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "    if not os.path.exists(dir + 'plots/'):\n",
    "        os.mkdir(dir + 'plots/')\n",
    "    if not os.path.exists(dir + 'cluster_tables/'):\n",
    "        os.mkdir(dir + 'cluster_tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to filter specified words  and pronouns\n",
    "\n",
    "def filter_words(sentence, words_to_remove):\n",
    "    # Tokenize\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # Tag each word with its part of speech\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    \n",
    "    # Define the pos tags for personal words\n",
    "    pronoun_tags = {'PRP', 'PRP$'}\n",
    "    \n",
    "    # list of words directly specified the user to remove\n",
    "    words_to_remove_lower = [noun.lower() for noun in words_to_remove]\n",
    "\n",
    "    #Filter out words that are personal words\n",
    "    filtered_words = [word for word, tag in pos_tags if tag not in pronoun_tags and word.lower() not in words_to_remove_lower]\n",
    "    \n",
    "    #Reassemble the sentence\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute embeddings for a single vignette\n",
    "\n",
    "def compute_embeddings(df, text_column, context_number, words_to_remove):\n",
    "    # select only responses from specified context\n",
    "    texts_df = df[df['context'] == context_number] \n",
    "    texts_df = texts_df.reset_index()\n",
    "\n",
    "    # Convert all responses to strings\n",
    "    texts_df[text_column] = texts_df[text_column].astype(str)\n",
    "\n",
    "    # Strips text of specified words and pronouns\n",
    "    texts_df[text_column] = texts_df[text_column].apply(lambda x: filter_words(x, words_to_remove))\n",
    "\n",
    "    texts = texts_df[text_column].tolist()\n",
    "\n",
    "    # compute option embeddings for each response\n",
    "    text_embeddings = model.encode(texts, show_progress_bar=True) \n",
    "    return text_embeddings, texts_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality reduction with pre-defined parameters\n",
    "def compute_clusters(text_embeddings, clustering_params, umap_params):\n",
    "    # initialize kmeans model\n",
    "    clustering_model = KMeans(n_clusters=clustering_params['num_clusters'], n_init='auto')\n",
    "\n",
    "    # reduce dimensionality of text embeddings\n",
    "    umap_embeddings = (umap.UMAP(n_neighbors=umap_params['n_neighbors'], \n",
    "                                    n_components=umap_params['n_components'], \n",
    "                                    metric=umap_params['metric'],\n",
    "                                    min_dist=umap_params['min_dist'],\n",
    "                                    random_state=umap_params['random_state'])\n",
    "                                .fit_transform(text_embeddings))\n",
    "    \n",
    "    # perform KMeans clustering and look at clusters \n",
    "    clustering_model.fit(umap_embeddings)\n",
    "    return umap_embeddings, clustering_model.labels_, clustering_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print clusters for row by row evaluation\n",
    "def print_clusters(texts_df, text_column_name, cluster_assignment, numb_clusters):\n",
    "    clustered_sentences = [[] for i in range(numb_clusters)]\n",
    "    for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "        clustered_sentences[cluster_id].append(texts_df[text_column_name][sentence_id])\n",
    "\n",
    "    for i, cluster in enumerate(clustered_sentences):\n",
    "        print(\"Cluster \", i+1)\n",
    "        cluster_list = []\n",
    "        for item in cluster:\n",
    "            # removes identical context that is included in text of each result\n",
    "            cluster_list.append(item.split(\":\", 1)[-1].strip())\n",
    "        print(cluster_list)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create elbow-plot to identify ideal number of clusters\n",
    "def k_plot(text_embeddings, umap_params, directory):\n",
    "    distortions = []\n",
    "    K_range = range(1, 18)\n",
    "\n",
    "\n",
    "    umap_embeddings = (umap.UMAP(n_neighbors=umap_params['n_neighbors'], \n",
    "                                        n_components=umap_params['n_components'], \n",
    "                                        metric=umap_params['metric'],\n",
    "                                        min_dist=umap_params['min_dist'],\n",
    "                                        random_state=umap_params['random_state'])\n",
    "                                    .fit_transform(text_embeddings))\n",
    "    \n",
    "    # perform KMeans for each k value\n",
    "    for k in K_range:\n",
    "        clustering_model = KMeans(n_clusters=k, n_init='auto')\n",
    "        clustering_model.fit(umap_embeddings)\n",
    "        distortions.append(clustering_model.inertia_) # appends inertia to distortians list\n",
    "\n",
    "    # Plotting the elbow curve\n",
    "    plt.plot(K_range, distortions, marker='o')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Sum of Squared Distances')\n",
    "    # saves figure to specified folder\n",
    "    plt.savefig(directory)\n",
    "    # plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df with clusters, centroids and umap embeddings\n",
    "def compute_centroid(texts_df, text_column_name, cluster_assignment, clustering_model, umap_embeddings):\n",
    "    # attach the cluster assignments to the dataframe\n",
    "    texts_df['cluster'] = pd.Series(cluster_assignment, index=texts_df.index)\n",
    "\n",
    "    # Attach the centroids to the dataframe\n",
    "    # In sklearn, the cluster centers are available directly via clustering_model.cluster_centers_\n",
    "    texts_df['centroid'] = texts_df['cluster'].apply(lambda x: clustering_model.cluster_centers_[x])\n",
    "\n",
    "    # Split the UMAP embeddings into individual columns for easier processing later\n",
    "    for i in range(umap_embeddings.shape[1]):\n",
    "        texts_df[f'umap_dim_{i}'] = umap_embeddings[:, i]\n",
    "\n",
    "    # Convert the UMAP embeddings from individual columns to lists for use in the distance calculation\n",
    "    umap_list = [f'umap_dim_{i}' for i in range(umap_embeddings.shape[1])]\n",
    "    texts_df['umap_embedding_list'] = texts_df[umap_list].apply(lambda row: row.tolist(), axis=1)\n",
    "\n",
    "    # Define a function to compute the distance of each embedding from its cluster's centroid\n",
    "    def distance_from_centroid(row):\n",
    "        return distance_matrix([row['umap_embedding_list']], [row['centroid']])[0][0]\n",
    "    texts_df['distance_from_centroid'] = texts_df.apply(distance_from_centroid, axis=1)\n",
    "\n",
    "    # Select the 'response' closest to each cluster centroid to serve as a summary of the cluster\n",
    "    summary = texts_df.sort_values('distance_from_centroid', ascending=True).groupby('cluster').head(1).sort_index()[text_column_name].tolist()\n",
    "\n",
    "    # Create a dictionary linking each summary 'response' to its corresponding cluster number\n",
    "    clusters = {}\n",
    "    for i in range(len(summary)):\n",
    "        clusters[summary[i]] = texts_df.loc[texts_df[text_column_name] == summary[i], \"cluster\"].iloc[0]\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimensionality of text vector embeddings to 2 for visualization\n",
    "def umap_2(texts_df, text_embeddings, clusters, umap_params):\n",
    "    umap_embeddings_2 = (umap.UMAP(n_neighbors=umap_params['n_neighbors'], \n",
    "                                    n_components=2, # only 2 dimensions\n",
    "                                    metric=umap_params['metric'],\n",
    "                                    min_dist=umap_params['min_dist'],\n",
    "                                    random_state=umap_params['random_state'])\n",
    "                                .fit_transform(text_embeddings))\n",
    "    for i in range(umap_embeddings_2.shape[1]):\n",
    "        texts_df[f'umap_dim2_{i}'] = umap_embeddings_2[:, i]\n",
    "\n",
    "    # create a dictionary with cluster numbers as keys and their centroid texts as values\n",
    "    id_to_name = {v: k for k, v in clusters.items()}\n",
    "\n",
    "    texts_df['cluster_name'] = texts_df['cluster'].map(id_to_name) # create a column with centroid texts\n",
    "\n",
    "    texts_df['cluster_name'] = texts_df['cluster_name'].apply(lambda x: x.split(\":\", 1)[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot clusters on the joint embedding space\n",
    "\n",
    "def plot_clusters(component1, component2, cluster, name, response,\n",
    "                   data_source, \n",
    "                   umap_params, clustering_params,\n",
    "                   plot_location):\n",
    "    pio.renderers.default = \"browser\"\n",
    "    color_palette = px.colors.qualitative.Light24\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    title_str = f\"UMAP Parameters: n_neighbors={umap_params['n_neighbors']}, n_components={umap_params['n_components']}, min_dist={umap_params['min_dist']} | Clustering: {clustering_params['algorithm_name']} with k={clustering_params['num_clusters']} clusters\"\n",
    "    \n",
    "    # Get unique clusters and data sources\n",
    "    unique_clusters = sorted(cluster.unique())\n",
    "    unique_data_sources = sorted(data_source.unique())\n",
    "\n",
    "    color_map = {uc: color_palette[i % len(color_palette)] for i, uc in enumerate(unique_clusters)}\n",
    "    marker_symbols = {\n",
    "        unique_data_sources[1]: 'square',\n",
    "        unique_data_sources[0]: 'diamond'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Add a trace for each Source to indicate the shape in the legend\n",
    "    for ds, symbol in marker_symbols.items():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                symbol=symbol,\n",
    "                # color=trajectory_colors[ds]\n",
    "            ),\n",
    "            name=f'{ds.capitalize()} (shape)'\n",
    "        ))\n",
    "\n",
    "    added_cluster_names = set()\n",
    "    for ds in unique_data_sources:\n",
    "        for uc in unique_clusters:\n",
    "            mask = (cluster == uc) & (data_source == ds)\n",
    "            if mask.any():  # Check if there are any rows after applying the mask\n",
    "                show_in_legend = name[mask].iloc[0] not in added_cluster_names\n",
    "                added_cluster_names.add(name[mask].iloc[0])\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=component1[mask],\n",
    "                    y=component2[mask],\n",
    "                    mode='text+markers',\n",
    "                    name=name[mask].iloc[0] if show_in_legend else None,\n",
    "                    legendgroup=f'group{uc}',\n",
    "                    showlegend=show_in_legend,\n",
    "                    hovertext= str(uc) + \": \" + response[mask] ,\n",
    "                    # text='gen_num[mask].astype(str)',\n",
    "                    marker=dict(\n",
    "                        size=12,\n",
    "                        color=color_map[uc],\n",
    "                        symbol=marker_symbols[ds],  # Use the marker symbol based on the data source\n",
    "                        line_width=1,\n",
    "                        opacity=1\n",
    "                    ),\n",
    "                    textfont=dict(\n",
    "                        size=10,\n",
    "                        color='black'\n",
    "                    )\n",
    "                ))\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=100, r=100, b=100, t=100),\n",
    "        width=2000,\n",
    "        height=1200,\n",
    "        showlegend=True,\n",
    "        title=title_str,\n",
    "        paper_bgcolor='white',  # White background for the entire plot area\n",
    "        plot_bgcolor='white',\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01,\n",
    "            bgcolor='rgba(255,255,255,0)'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.layout.template = 'ggplot2'\n",
    "    fig.write_html(plot_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_list = ['Heinz', 'Josh', 'Brian', 'Liz', 'Mary', 'Brad', 'Darya', 'Eunice', 'Eamon', 'Cameron', 'Erica', 'Carl', 'Daniel', 'Andy', 'Ahmed', 'Eva', 'Jeff', 'Shania']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load first-person decision study data\n",
    "df_decision = pd.read_csv('../data/decision.csv')\n",
    "\n",
    "# add id column\n",
    "df_decision = df_decision.reset_index().rename(columns={'index': 'id'})\n",
    "df_decision['id'] += 1\n",
    "\n",
    "# select only participants who finished\n",
    "df_decision = df_decision[df_decision['finished']]\n",
    "\n",
    "# exclude ids of participants who gave non-sensical responses\n",
    "exclude_ids = [21, 64, 72, 74, 84, 86, 89]\n",
    "df_decision[~df_decision['id'].isin(exclude_ids)].reset_index(drop=True) \n",
    "\n",
    "# Select columns 'id', 'S1_1' to 'S18_1'\n",
    "df_decision = df_decision[['id', 'S1_1', 'S2_1', 'S3_1', 'S4_1', 'S5_1', 'S6_1', 'S7_1', 'S8_1', 'S9_1', 'S10_1', 'S11_1', 'S12_1', 'S13_1', 'S14_1', 'S15_1', 'S16_1', 'S17_1', 'S18_1']]\n",
    "\n",
    "#  Melt the DataFrame to long format\n",
    "df_decision = pd.melt(df_decision, id_vars=['id'], var_name='context', value_name='decision')\n",
    "\n",
    "# Extract numeric values from 'context' column using str.extract\n",
    "df_decision['context'] = df_decision['context'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# drop empty responses\n",
    "df_decision.dropna(subset=['decision'], inplace=True)\n",
    "df_decision.rename(columns={'decision': 'response'}, inplace=True)\n",
    "\n",
    "# add source column indicating that this is from the decision study\n",
    "df_decision['source'] = 'decision'\n",
    "\n",
    "# df_decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load third-person possibility generation study data\n",
    "df_pg = pd.read_csv('../manualCoding/pg_coded_final.csv', index_col=0)\n",
    "df_pg = df_pg[['context', 'id', 'answer', 'text', 'value']]\n",
    "df_pg.rename(columns={\"text\":\"response\"}, inplace=True)\n",
    "df_pg['source'] = 'pg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge decision study data and possibility generation study data into one data frame for clustering\n",
    "df = pd.merge(df_decision, df_pg, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add full scenario texts to merged_text to give LLM context for responses\n",
    "contexts_list = pd.read_csv('../materials/contextsTable.csv', index_col=0)['text']\n",
    "\n",
    "df_merge = pd.merge(df, contexts_list, left_on='context', right_index=True)\n",
    "df_merge['merged_text'] = df_merge['text'] + ' : ' + df_merge['response']\n",
    "df_merge.rename(columns={\"response\": \"response_original\", \"text\": \"scenario_text\"}, inplace=True)\n",
    "\n",
    "df = df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create elbow plots to determine appropriate number of clusters per context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_column_name texts: 'response_original' or 'merged_text'\n",
    "# merged_text gives context by merging response with the scenario text and is preferred\n",
    "text_column_name = 'merged_text'\n",
    "\n",
    "\n",
    "# directory to place resulting plots and clusters\n",
    "dir = \"numElbow/\"\n",
    "\n",
    "# check that path exists and make it if it doesn't\n",
    "if not os.path.exists(dir + 'elbow_plots/'):\n",
    "    if not os.path.exists(dir):\n",
    "            os.mkdir(dir)\n",
    "    os.mkdir(dir + 'elbow_plots/')\n",
    "\n",
    "# Define UMAP and clustering parameters\n",
    "umap_params = {'n_neighbors': 100, 'n_components': 10, 'metric': 'cosine', 'min_dist': 0.05, 'random_state': None}\n",
    "\n",
    "# creates plots and cluster_tables for each scenario\n",
    "for scenario_number in range(1, 19):\n",
    "    print(\"Creating k-plot for scenario \" + str(scenario_number))\n",
    "    # selects appropriate agent name based on scenario number\n",
    "    agent_name = agent_list[scenario_number-1]\n",
    "    words_to_remove = [agent_name, 'should', 'would', 'could']\n",
    "\n",
    "    text_embeddings, texts_df = compute_embeddings(df, text_column_name, scenario_number, words_to_remove)\n",
    "\n",
    "    # elbow curve plots for determining appropriate number of clusters\n",
    "    k_plot(text_embeddings, umap_params, f'{dir}elbow_plots/S{str(scenario_number)}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on visual examination of the resulting plots from the code above, optimal k values were selected and stored in elbow_results.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all scenarios in loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To run the analyses edit the values in this cell then run this cell and the one below\n",
    "# For the repo this was run twice, once with dir = \"numManual/\" and clusters_from = 'same_as_manual',\n",
    "# and once with dir = \"numElbow/\" and clusters_from = 'elbow_results',\n",
    "\n",
    "\n",
    "# directory to send resulting plots and clusters\n",
    "dir = \"numManual/\"\n",
    "\n",
    "## text_column_name texts: 'response_original' or 'merged_text'\n",
    "# merged_text gives context by merging response with the scenario text and generally gives better results\n",
    "text_column_name = 'merged_text'\n",
    "\n",
    "## clusters_from texts: 'elbow_results', 'same_as_manual', 'fixed'\n",
    "# 'elbow_results' use the number of clusters for each context based on the results of the elbow plots (code below)\n",
    "# 'same_as_manual' uses the same number of clusters for each context as was determine via manual coding for fair comparison between the two\n",
    "# 'fixed' allows you to directly set the number of clusters\n",
    "clusters_from = 'same_as_manual'\n",
    "\n",
    "if clusters_from == 'fixed':\n",
    "    # Modify this if fixing the number of clusters\n",
    "    numb_clusters = 11\n",
    "\n",
    "# Define UMAP and clustering parameters\n",
    "umap_params = {'n_neighbors': 100, 'n_components': 10, 'metric': 'cosine', 'min_dist': 0.05, 'random_state': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of groups used for manual coding each scenario\n",
    "num_manual_groups_by_scenario = [14, 16, 15, 16, 14, 15, 13, 15, 16, 14, 15, 17, 17, 18, 17, 14, 14, 14]\n",
    "\n",
    "# optimal number of clusters for each scenario as determined by elbow plot analysis above\n",
    "if clusters_from == 'elbow_results':\n",
    "    with open(dir +'/elbow_results.json', \"r\") as json_file:\n",
    "        elbow_results = json.load(json_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that path exists and make it if it doesn't\n",
    "make_directories(dir)\n",
    "\n",
    "# creates plots and cluster_tables for each scenario\n",
    "for scenario_number in range(1, 19):\n",
    "    print(\"Running analysis on scenario \" + str(scenario_number))\n",
    "    # selects appropriate agent name based on scenario number\n",
    "    agent_list = ['Heinz', 'Josh', 'Brian', 'Liz', 'Mary', 'Brad', 'Darya', 'Eunice', 'Eamon', 'Cameron', 'Erica', 'Carl', 'Daniel', 'Andy', 'Ahmed', 'Eva', 'Jeff', 'Shania']\n",
    "    agent_name = agent_list[scenario_number-1]\n",
    "\n",
    "\n",
    "    words_to_remove = [agent_name, 'should', 'would', 'could']\n",
    "\n",
    "    # Strip each response of words_to_remove and compute SBERT embeddings\n",
    "    text_embeddings, texts_df = compute_embeddings(df, text_column_name, scenario_number, words_to_remove)\n",
    "\n",
    "\n",
    "    if(clusters_from == \"elbow_results\"):\n",
    "        numb_clusters = elbow_results[str(scenario_number)]\n",
    "\n",
    "    # select appropriate number of clusters if the numbers of clusters is the same as manual coding\n",
    "    if clusters_from == 'same_as_manual':\n",
    "        # + 1 is added to above number because some texts were manually coded as 'other', giving one more category than the stated number\n",
    "        numb_clusters = num_manual_groups_by_scenario[scenario_number-1]+1\n",
    "\n",
    "    # define clustering params based on numb_clusters defined above\n",
    "    clustering_params = {'algorithm_name': 'KMeans', 'num_clusters': numb_clusters}\n",
    "    \n",
    "    # compute clusters on embedded \n",
    "    umap_embeddings, cluster_assignment, clustering_model = compute_clusters(text_embeddings, clustering_params, umap_params)\n",
    "\n",
    "\n",
    "    # print_clusters(texts_df, text_column_name, cluster_assignment, numb_clusters)\n",
    "\n",
    "    # create a df with clusters, centroids and umap embeddings\n",
    "    clusters = compute_centroid(texts_df, text_column_name, cluster_assignment, clustering_model, umap_embeddings)\n",
    "    # reduce dimensionality to two for plotting\n",
    "    umap_2(texts_df, text_embeddings, clusters, umap_params)\n",
    "\n",
    "\n",
    "    # save texts_df for this scenario to cluster_tables\n",
    "    texts_df.to_csv(dir + 'cluster_tables/S' + str(scenario_number) + '.csv')\n",
    "    # Call your function with the appropriate DataFrame columns\n",
    "    plot_clusters(\n",
    "        texts_df['umap_dim2_0'],\n",
    "        texts_df['umap_dim2_1'],\n",
    "        texts_df['cluster'],\n",
    "        texts_df[\"cluster_name\"],\n",
    "        texts_df[\"response_original\"],\n",
    "        texts_df[\"source\"],\n",
    "        umap_params,\n",
    "        clustering_params,\n",
    "        f'{dir}plots/S{scenario_number}.html'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
